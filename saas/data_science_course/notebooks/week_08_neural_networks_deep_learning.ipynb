{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand neural network fundamentals\n",
    "- Understand how to use TensorFlow/Keras\n",
    "- Understand pros/cons of standard Neural Networks, CNN, RNN, LSTM, GNN\n",
    "\n",
    "## Topics Covered:\n",
    "- Perceptron and multi-layer perceptrons (lightly)\n",
    "- Activation functions (lightly)\n",
    "- Backpropagation algorithm (lightly)\n",
    "- Gradient descent optimization (lightly)\n",
    "- Introduction to TensorFlow/Keras\n",
    "- Convolutional Neural Networks (CNN) basics\n",
    "- Recurrent Neural Networks (RNN) basics\n",
    "- Long-Short Term Memory Neural Networks (LSTM) basics\n",
    "- Graph Neural Network (GNN) basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning libraries\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models, optimizers\n",
    "    from tensorflow.keras.datasets import mnist, cifar10\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(\"TensorFlow/Keras available\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available - install with: pip install tensorflow\")\n",
    "    tf = None\n",
    "    keras = None\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if tf is not None:\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Fundamentals\n",
    "\n",
    "Neural networks are inspired by the human brain and consist of interconnected nodes (neurons) that process information.\n",
    "\n",
    "### Key Components:\n",
    "- **Neurons**: Basic processing units\n",
    "- **Weights**: Connection strengths between neurons\n",
    "- **Biases**: Threshold adjustments\n",
    "- **Activation Functions**: Non-linear transformations\n",
    "- **Layers**: Groups of neurons\n",
    "\n",
    "### Types of Neural Networks:\n",
    "1. **Perceptron**: Single neuron, linear classifier\n",
    "2. **Multi-layer Perceptron (MLP)**: Fully connected layers\n",
    "3. **Convolutional Neural Networks (CNN)**: For image processing\n",
    "4. **Recurrent Neural Networks (RNN)**: For sequential data\n",
    "5. **Long Short-Term Memory (LSTM)**: Advanced RNN\n",
    "6. **Graph Neural Networks (GNN)**: For graph-structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a simple neural network\n",
    "def plot_neural_network():\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Define network structure\n",
    "    layers = [3, 4, 4, 2]  # Input, Hidden1, Hidden2, Output\n",
    "    layer_names = ['Input\\nLayer', 'Hidden\\nLayer 1', 'Hidden\\nLayer 2', 'Output\\nLayer']\n",
    "    \n",
    "    # Calculate positions\n",
    "    max_neurons = max(layers)\n",
    "    layer_positions = np.arange(len(layers))\n",
    "    \n",
    "    # Plot neurons\n",
    "    for i, (layer_size, layer_name) in enumerate(zip(layers, layer_names)):\n",
    "        neuron_positions = np.linspace(0, max_neurons-1, layer_size)\n",
    "        neuron_positions = neuron_positions - np.mean(neuron_positions) + (max_neurons-1)/2\n",
    "        \n",
    "        for j, pos in enumerate(neuron_positions):\n",
    "            circle = plt.Circle((layer_positions[i], pos), 0.15, \n",
    "                              color='lightblue', ec='black', linewidth=1.5)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Add connections to next layer\n",
    "            if i < len(layers) - 1:\n",
    "                next_layer_positions = np.linspace(0, max_neurons-1, layers[i+1])\n",
    "                next_layer_positions = next_layer_positions - np.mean(next_layer_positions) + (max_neurons-1)/2\n",
    "                \n",
    "                for next_pos in next_layer_positions:\n",
    "                    ax.plot([layer_positions[i] + 0.15, layer_positions[i+1] - 0.15], \n",
    "                           [pos, next_pos], 'k-', alpha=0.3, linewidth=0.8)\n",
    "        \n",
    "        # Add layer labels\n",
    "        ax.text(layer_positions[i], -0.8, layer_name, \n",
    "                ha='center', va='top', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(-0.5, len(layers)-0.5)\n",
    "    ax.set_ylim(-1.2, max_neurons-0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Multi-Layer Perceptron (MLP) Architecture', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns.\n",
    "\n",
    "### Common Activation Functions:\n",
    "- **Sigmoid**: Ïƒ(x) = 1/(1+e^(-x)) - Outputs between 0 and 1\n",
    "- **Tanh**: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x)) - Outputs between -1 and 1\n",
    "- **ReLU**: f(x) = max(0, x) - Most popular, simple and effective\n",
    "- **Leaky ReLU**: f(x) = max(0.01x, x) - Addresses dying ReLU problem\n",
    "- **Softmax**: For multi-class classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "def plot_activation_functions():\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # Define activation functions\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    tanh = np.tanh(x)\n",
    "    relu = np.maximum(0, x)\n",
    "    leaky_relu = np.maximum(0.01 * x, x)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Sigmoid\n",
    "    axes[0, 0].plot(x, sigmoid, 'b-', linewidth=2, label='Sigmoid')\n",
    "    axes[0, 0].set_title('Sigmoid Activation')\n",
    "    axes[0, 0].set_xlabel('Input')\n",
    "    axes[0, 0].set_ylabel('Output')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Tanh\n",
    "    axes[0, 1].plot(x, tanh, 'r-', linewidth=2, label='Tanh')\n",
    "    axes[0, 1].set_title('Tanh Activation')\n",
    "    axes[0, 1].set_xlabel('Input')\n",
    "    axes[0, 1].set_ylabel('Output')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # ReLU\n",
    "    axes[1, 0].plot(x, relu, 'g-', linewidth=2, label='ReLU')\n",
    "    axes[1, 0].set_title('ReLU Activation')\n",
    "    axes[1, 0].set_xlabel('Input')\n",
    "    axes[1, 0].set_ylabel('Output')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Leaky ReLU\n",
    "    axes[1, 1].plot(x, leaky_relu, 'purple', linewidth=2, label='Leaky ReLU')\n",
    "    axes[1, 1].set_title('Leaky ReLU Activation')\n",
    "    axes[1, 1].set_xlabel('Input')\n",
    "    axes[1, 1].set_ylabel('Output')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_activation_functions()\n",
    "\n",
    "# Print characteristics\n",
    "print(\"Activation Function Characteristics:\")\n",
    "print(\"- Sigmoid: Smooth, bounded [0,1], vanishing gradient problem\")\n",
    "print(\"- Tanh: Smooth, bounded [-1,1], zero-centered, vanishing gradient problem\")\n",
    "print(\"- ReLU: Simple, unbounded, efficient, dying ReLU problem\")\n",
    "print(\"- Leaky ReLU: Addresses dying ReLU, allows small negative values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Neural Networks with TensorFlow/Keras\n",
    "\n",
    "TensorFlow is a popular deep learning framework, and Keras is its high-level API that makes building neural networks intuitive.\n",
    "\n",
    "### Key Components:\n",
    "- **Sequential Model**: Linear stack of layers\n",
    "- **Functional API**: More flexible model building\n",
    "- **Layers**: Dense, Conv2D, LSTM, etc.\n",
    "- **Optimizers**: Adam, SGD, RMSprop\n",
    "- **Loss Functions**: MSE, categorical_crossentropy, binary_crossentropy\n",
    "- **Metrics**: Accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf is not None:\n",
    "    # Create a simple dataset for demonstration\n",
    "    print(\"=== CREATING NEURAL NETWORK WITH TENSORFLOW/KERAS ===\")\n",
    "    \n",
    "    # Generate synthetic classification data\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                              n_redundant=5, n_classes=3, random_state=42)\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=3)\n",
    "    \n",
    "    print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Training labels shape: {y_train_cat.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "    \n",
    "    # Build a simple neural network\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "        layers.Dropout(0.3),  # Regularization\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(3, activation='softmax')  # Output layer\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Display model architecture\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping neural network examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf is not None:\n",
    "    # Train the model\n",
    "    print(\"=== TRAINING THE NEURAL NETWORK ===\")\n",
    "    \n",
    "    # Train with validation split\n",
    "    history = model.fit(X_train_scaled, y_train_cat,\n",
    "                       epochs=50,\n",
    "                       batch_size=32,\n",
    "                       validation_split=0.2,\n",
    "                       verbose=1)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
    "    print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test_scaled)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf is not None:\n",
    "    # Plot training history\n",
    "    def plot_training_history(history):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot training & validation accuracy\n",
    "        axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        axes[0].set_title('Model Accuracy')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot training & validation loss\n",
    "        axes[1].plot(history.history['loss'], label='Training Loss')\n",
    "        axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "        axes[1].set_title('Model Loss')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "                yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are specialized neural networks for processing grid-like data such as images.\n",
    "\n",
    "### Key Components:\n",
    "- **Convolutional Layers**: Apply filters to detect features\n",
    "- **Pooling Layers**: Reduce spatial dimensions\n",
    "- **Filters/Kernels**: Small matrices that detect patterns\n",
    "- **Feature Maps**: Outputs of convolutional operations\n",
    "- **Padding**: Control output size\n",
    "- **Stride**: Step size of convolution\n",
    "\n",
    "### Advantages:\n",
    "- Translation invariance\n",
    "- Parameter sharing\n",
    "- Hierarchical feature learning\n",
    "- Excellent for image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf is not None:\n",
    "    # Load and preprocess MNIST dataset\n",
    "    print(\"=== CONVOLUTIONAL NEURAL NETWORK (CNN) ===\")\n",
    "    \n",
    "    # Load MNIST data\n",
    "    (X_train_img, y_train_img), (X_test_img, y_test_img) = mnist.load_data()\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    X_train_img = X_train_img.astype('float32') / 255.0\n",
    "    X_test_img = X_test_img.astype('float32') / 255.0\n",
    "    \n",
    "    # Reshape for CNN (add channel dimension)\n",
    "    X_train_img = X_train_img.reshape(-1, 28, 28, 1)\n",
    "    X_test_img = X_test_img.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    y_train_img = to_categorical(y_train_img, 10)\n",
    "    y_test_img = to_categorical(y_test_img, 10)\n",
    "    \n",
    "    print(f\"Training images shape: {X_train_img.shape}\")\n",
    "    print(f\"Training labels shape: {y_train_img.shape}\")\n",
    "    \n",
    "    # Visualize some samples\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    for i in range(10):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        axes[row, col].imshow(X_train_img[i].reshape(28, 28), cmap='gray')\n",
    "        axes[row, col].set_title(f'Label: {np.argmax(y_train_img[i])}')\n",
    "        axes[row, col].axis('off')\n",
    "    plt.suptitle('Sample MNIST Images')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping CNN example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf is not None:\n",
    "    # Build CNN model\n",
    "    cnn_model = models.Sequential([\n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the CNN\n",
    "    cnn_model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    # Display model architecture\n",
    "    print(\"\\nCNN Model Architecture:\")\n",
    "    cnn_model.summary()\n",
    "    \n",
    "    # Train the CNN (using a subset for speed)\n",
    "    print(\"\\nTraining CNN...\")\n",
    "    cnn_history = cnn_model.fit(X_train_img[:5000], y_train_img[:5000],\n",
    "                                epochs=10,\n",
    "                                batch_size=128,\n",
    "                                validation_split=0.2,\n",
    "                                verbose=1)\n",
    "    \n",
    "    # Evaluate CNN\n",
    "    cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(X_test_img[:1000], y_test_img[:1000], verbose=0)\n",
    "    print(f\"\\nCNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping CNN training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are designed for sequential data where the order of inputs matters.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Memory**: Hidden state carries information from previous time steps\n",
    "- **Sequential Processing**: Process one element at a time\n",
    "- **Shared Parameters**: Same weights used at each time step\n",
    "- **Variable Length**: Can handle sequences of different lengths\n",
    "\n",
    "### Applications:\n",
    "- Natural Language Processing\n",
    "- Time series prediction\n",
    "- Speech recognition\n",
    "- Machine translation\n",
    "\n",
    "### Limitations:\n",
    "- Vanishing gradient problem\n",
    "- Difficulty capturing long-term dependencies\n",
    "- Sequential processing (not parallelizable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf is not None:\n",
    "    # Generate synthetic time series data\n",
    "    print(\"=== RECURRENT NEURAL NETWORK (RNN) ===\")\n",
    "    \n",
    "    # Create sine wave time series\n",
    "    def generate_time_series(n_samples=1000, sequence_length=50):\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Generate sine wave with some noise\n",
    "            start = np.random.uniform(0, 4*np.pi)\n",
"            t = np.linspace(start, start + 2*np.pi, sequence_length + 1)\n",
"            series = np.sin(t) + 0.1 * np.random.normal(0, 1, sequence_length + 1)\n",
"            \n",
"            # Use first 'sequence_length' points as input, next point as target\n",
"            X.append(series[:-1])\n",
"            y.append(series[-1])\n",
"        \n",
"        return np.array(X), np.array(y)\n",
"    \n",
"    # Generate time series data\n",
"    X_ts, y_ts = generate_time_series(n_samples=1000, sequence_length=20)\n",
"    \n",
"    # Reshape for RNN (samples, time_steps, features)\n",
"    X_ts = X_ts.reshape(X_ts.shape[0], X_ts.shape[1], 1)\n",
"    \n",
"    # Split data\n",
"    X_train_ts, X_test_ts, y_train_ts, y_test_ts = train_test_split(X_ts, y_ts, test_size=0.2, random_state=42)\n",
"    \n",
"    print(f\"Time series data shape: {X_train_ts.shape}\")\n",
"    print(f\"Time series labels shape: {y_train_ts.shape}\")\n",
"    \n",
"    # Visualize sample time series\n",
"    plt.figure(figsize=(12, 6))\n",
"    for i in range(3):\n",
"        plt.subplot(1, 3, i+1)\n",
"        plt.plot(X_ts[i].flatten(), 'b-', label='Input sequence')\n",
"        plt.axhline(y=y_ts[i], color='r', linestyle='--', label='Target')\n",
"        plt.title(f'Time Series {i+1}')\n",
"        plt.xlabel('Time Step')\n",
"        plt.ylabel('Value')\n",
"        plt.legend()\n",
"        plt.grid(True, alpha=0.3)\n",
"    \n",
"    plt.tight_layout()\n",
"    plt.show()\n",
"    \n",
"else:\n",
"    print(\"TensorFlow not available - skipping RNN example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf is not None:\n",
    "    # Build simple RNN model\n",
    "    rnn_model = models.Sequential([\n",
    "        layers.SimpleRNN(50, activation='relu', input_shape=(20, 1)),\n",
    "        layers.Dense(25, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile RNN\n",
    "    rnn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    print(\"\\nRNN Model Architecture:\")\n",
    "    rnn_model.summary()\n",
    "    \n",
    "    # Train RNN\n",
    "    print(\"\\nTraining RNN...\")\n",
    "    rnn_history = rnn_model.fit(X_train_ts, y_train_ts,\n",
    "                               epochs=20,\n",
    "                               batch_size=32,\n",
    "                               validation_split=0.2,\n",
    "                               verbose=1)\n",
    "    \n",
    "    # Evaluate RNN\n",
    "    rnn_test_loss, rnn_test_mae = rnn_model.evaluate(X_test_ts, y_test_ts, verbose=0)\n",
    "    print(f\"\\nRNN Test Loss: {rnn_test_loss:.4f}\")\n",
    "    print(f\"RNN Test MAE: {rnn_test_mae:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping RNN training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Long Short-Term Memory (LSTM) Networks\n",
    "\n",
    "LSTMs are a special type of RNN designed to overcome the vanishing gradient problem and capture long-term dependencies.\n",
    "\n",
    "### Key Components:\n",
    "- **Cell State**: Long-term memory\n",
    "- **Hidden State**: Short-term memory\n",
    "- **Gates**: Control information flow\n",
    "  - **Forget Gate**: Decides what to forget from cell state\n",
    "  - **Input Gate**: Decides what new information to store\n",
    "  - **Output Gate**: Decides what to output\n",
    "\n",
    "### Advantages over RNNs:\n",
    "- Better at capturing long-term dependencies\n",
    "- Reduced vanishing gradient problem\n",
    "- More stable training\n",
    "- Better performance on complex sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf is not None:\n",
    "    # Build LSTM model\n",
    "    print(\"=== LONG SHORT-TERM MEMORY (LSTM) ===\")\n",
    "    \n",
    "    lstm_model = models.Sequential([\n",
    "        layers.LSTM(50, activation='relu', input_shape=(20, 1)),\n",
    "        layers.Dense(25, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile LSTM\n",
    "    lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    print(\"\\nLSTM Model Architecture:\")\n",
    "    lstm_model.summary()\n",
    "    \n",
    "    # Train LSTM\n",
    "    print(\"\\nTraining LSTM...\")\n",
    "    lstm_history = lstm_model.fit(X_train_ts, y_train_ts,\n",
    "                                 epochs=20,\n",
    "                                 batch_size=32,\n",
    "                                 validation_split=0.2,\n",
    "                                 verbose=1)\n",
    "    \n",
    "    # Evaluate LSTM\n",
    "    lstm_test_loss, lstm_test_mae = lstm_model.evaluate(X_test_ts, y_test_ts, verbose=0)\n",
    "    print(f\"\\nLSTM Test Loss: {lstm_test_loss:.4f}\")\n",
    "    print(f\"LSTM Test MAE: {lstm_test_mae:.4f}\")\n",
    "    \n",
    "    # Compare RNN vs LSTM\n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"RNN Test MAE: {rnn_test_mae:.4f}\")\n",
    "    print(f\"LSTM Test MAE: {lstm_test_mae:.4f}\")\n",
    "    print(f\"LSTM improvement: {((rnn_test_mae - lstm_test_mae) / rnn_test_mae * 100):.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping LSTM example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Graph Neural Networks (GNNs) - Basics\n",
    "\n",
    "GNNs are designed to work with graph-structured data where relationships between entities are as important as the entities themselves.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Nodes**: Entities in the graph\n",
    "- **Edges**: Relationships between entities\n",
    "- **Node Features**: Properties of entities\n",
    "- **Edge Features**: Properties of relationships\n",
    "- **Message Passing**: Nodes exchange information with neighbors\n",
    "- **Aggregation**: Combining information from neighbors\n",
    "\n",
    "### Applications:\n",
    "- Social network analysis\n",
    "- Molecular property prediction\n",
    "- Recommendation systems\n",
    "- Knowledge graphs\n",
    "- Traffic prediction\n",
    "\n",
    "### Advantages:\n",
    "- Naturally handles non-Euclidean data\n",
    "- Captures relational information\n",
    "- Flexible for various graph sizes\n",
    "- Good for semi-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GNN concept demonstration (without specialized libraries)\n",
    "print(\"=== GRAPH NEURAL NETWORK (GNN) CONCEPTS ===\")\n",
    "\n",
    "# Create a simple graph representation\n",
    "# Node features: [node_id, feature1, feature2]\n",
    "# Edge list: [(source, target), ...]\n",
    "\n",
    "# Example: Simple social network\n",
    "nodes = {\n",
    "    0: {'features': [1.0, 0.5], 'label': 'Person A'},\n",
    "    1: {'features': [0.8, 0.3], 'label': 'Person B'},\n",
    "    2: {'features': [0.2, 0.9], 'label': 'Person C'},\n",
    "    3: {'features': [0.6, 0.7], 'label': 'Person D'},\n",
    "    4: {'features': [0.4, 0.1], 'label': 'Person E'}\n",
    "}\n",
    "\n",
    "edges = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4)]\n",
    "\n",
    "# Create adjacency matrix\n",
    "n_nodes = len(nodes)\n",
    "adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "for source, target in edges:\n",
    "    adj_matrix[source, target] = 1\n",
    "    adj_matrix[target, source] = 1  # Undirected graph\n",
    "\n",
    "# Extract node features\n",
    "node_features = np.array([nodes[i]['features'] for i in range(n_nodes)])\n",
    "\n",
    "print(f\"Number of nodes: {n_nodes}\")\n",
    "print(f\"Number of edges: {len(edges)}\")\n",
    "print(f\"Node features shape: {node_features.shape}\")\n",
    "print(f\"Adjacency matrix shape: {adj_matrix.shape}\")\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot nodes\n",
    "pos = {0: (0, 1), 1: (1, 2), 2: (2, 1), 3: (1, 0), 4: (0, -1)}\n",
    "for node_id, (x, y) in pos.items():\n",
    "    plt.scatter(x, y, s=300, c='lightblue', edgecolors='black', linewidth=2)\n",
    "    plt.text(x, y, nodes[node_id]['label'], ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Plot edges\n",
    "for source, target in edges:\n",
    "    x1, y1 = pos[source]\n",
    "    x2, y2 = pos[target]\n",
    "    plt.plot([x1, x2], [y1, y2], 'k-', alpha=0.5, linewidth=2)\n",
    "\n",
    "plt.title('Sample Graph Structure')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGNN Processing Steps:\")\n",
    "print(\"1. Initialize node features\")\n",
    "print(\"2. For each layer:\")\n",
    "print(\"   - Aggregate features from neighbors\")\n",
    "print(\"   - Update node features using aggregated information\")\n",
    "print(\"3. Use final node features for prediction\")\n",
    "\n",
    "# Simple message passing example\n",
    "print(\"\\nSimple Message Passing Example:\")\n",
    "print(\"Original node features:\")\n",
    "for i, features in enumerate(node_features):\n",
    "    print(f\"Node {i}: {features}\")\n",
    "\n",
    "# Aggregate neighbors (mean aggregation)\n",
    "new_features = np.zeros_like(node_features)\n",
    "for i in range(n_nodes):\n",
    "    neighbors = np.where(adj_matrix[i] == 1)[0]\n",
    "    if len(neighbors) > 0:\n",
    "        # Average of neighbor features\n",
    "        neighbor_features = node_features[neighbors]\n",
    "        aggregated = np.mean(neighbor_features, axis=0)\n",
    "        # Combine with own features\n",
    "        new_features[i] = 0.5 * node_features[i] + 0.5 * aggregated\n",
    "    else:\n",
    "        new_features[i] = node_features[i]\n",
    "\n",
    "print(\"\\nAfter one message passing step:\")\n",
    "for i, features in enumerate(new_features):\n",
    "    print(f\"Node {i}: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Neural Network Comparison\n",
    "\n",
    "Let's compare the different types of neural networks we've covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Comparison\n",
    "print(\"=== NEURAL NETWORK COMPARISON ===\")\n",
    "\n",
    "comparison_data = {\n",
    "    'Network Type': ['MLP', 'CNN', 'RNN', 'LSTM', 'GNN'],\n",
    "    'Best For': [\n",
    "        'Tabular data, general classification',\n",
    "        'Images, spatial data',\n",
    "        'Sequential data, time series',\n",
    "        'Long sequences, NLP',\n",
    "        'Graph data, relationships'\n",
    "    ],\n",
    "    'Key Strength': [\n",
    "        'Universal approximator',\n",
    "        'Translation invariance',\n",
    "        'Handles sequences',\n",
    "        'Long-term memory',\n",
    "        'Relational reasoning'\n",
    "    ],\n",
    "    'Main Weakness': [\n",
    "        'No structure awareness',\n",
    "        'Large parameter count',\n",
    "        'Vanishing gradients',\n",
    "        'Computational complexity',\n",
    "        'Complex implementation'\n",
    "    ],\n",
    "    'Typical Use Cases': [\n",
    "        'Fraud detection, credit scoring',\n",
    "        'Image classification, object detection',\n",
    "        'Stock prediction, simple NLP',\n",
    "        'Machine translation, chatbots',\n",
    "        'Social networks, drug discovery'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nNeural Network Comparison:\")\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"\\n{row['Network Type']}:\")\n",
    "    print(f\"  Best For: {row['Best For']}\")\n",
    "    print(f\"  Key Strength: {row['Key Strength']}\")\n",
    "    print(f\"  Main Weakness: {row['Main Weakness']}\")\n",
    "    print(f\"  Typical Use Cases: {row['Typical Use Cases']}\")\n",
    "\n",
    "# Create a visual comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Create a heatmap-like visualization\n",
    "categories = ['Tabular Data', 'Image Data', 'Sequential Data', 'Graph Data', 'Interpretability']\n",
    "networks = ['MLP', 'CNN', 'RNN', 'LSTM', 'GNN']\n",
    "\n",
    "# Suitability scores (1-5 scale)\n",
    "scores = np.array([\n",
    "    [5, 3, 3, 2, 4],  # MLP\n",
    "    [3, 5, 2, 1, 3],  # CNN\n",
    "    [2, 2, 4, 1, 3],  # RNN\n",
    "    [2, 2, 5, 1, 2],  # LSTM\n",
    "    [3, 2, 3, 5, 2]   # GNN\n",
    "])\n",
    "\n",
    "im = ax.imshow(scores, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# Add labels\n",
    "ax.set_xticks(range(len(categories)))\n",
    "ax.set_yticks(range(len(networks)))\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_yticklabels(networks)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(networks)):\n",
    "    for j in range(len(categories)):\n",
    "        text = ax.text(j, i, scores[i, j], ha='center', va='center', \n",
    "                      color='white' if scores[i, j] > 3 else 'black', fontweight='bold')\n",
    "\n",
    "ax.set_title('Neural Network Suitability by Data Type (1-5 Scale)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, label='Suitability Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Congratulations! You've completed your introduction to neural networks and deep learning. Here's what you learned:\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "1. **Neural Network Fundamentals**: Neurons, weights, biases, activation functions\n",
    "2. **Multi-layer Perceptrons (MLPs)**: Fully connected networks for general tasks\n",
    "3. **Convolutional Neural Networks (CNNs)**: Specialized for image processing\n",
    "4. **Recurrent Neural Networks (RNNs)**: For sequential data processing\n",
    "5. **Long Short-Term Memory (LSTM)**: Advanced RNNs for long-term dependencies\n",
    "6. **Graph Neural Networks (GNNs)**: For graph-structured data\n",
    "7. **TensorFlow/Keras**: Deep learning framework and high-level API\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- Understanding different neural network architectures\n",
    "- Building and training neural networks with TensorFlow/Keras\n",
    "- Choosing appropriate architectures for different data types\n",
    "- Understanding the strengths and weaknesses of each approach\n",
    "- Implementing basic deep learning workflows\n",
    "- Evaluating neural network performance\n",
    "\n",
    "### When to Use Each Architecture:\n",
    "- **MLP**: Tabular data, general classification/regression\n",
    "- **CNN**: Images, spatial data, pattern recognition\n",
    "- **RNN**: Sequential data, simple time series\n",
    "- **LSTM**: Complex sequences, NLP, long-term dependencies\n",
    "- **GNN**: Graph data, social networks, molecular structures\n",
    "\n",
    "### Best Practices:\n",
    "- Start with simpler architectures before moving to complex ones\n",
    "- Always normalize/scale your input data\n",
    "- Use appropriate activation functions for your task\n",
    "- Monitor training to avoid overfitting\n",
    "- Use dropout and regularization techniques\n",
    "- Experiment with different optimizers and learning rates\n",
    "\n",
    "### Real-world Applications:\n",
    "- **Computer Vision**: Image classification, object detection, medical imaging\n",
    "- **Natural Language Processing**: Translation, sentiment analysis, chatbots\n",
    "- **Time Series**: Stock prediction, weather forecasting, sensor data\n",
    "- **Recommendation Systems**: Content filtering, collaborative filtering\n",
    "- **Game Playing**: Chess, Go, video games\n",
    "- **Scientific Research**: Drug discovery, protein folding, climate modeling\n",
    "\n",
    "### Next Steps:\n",
    "In the next week, we'll dive deeper into modern AI applications, focusing on large language models, transformers, and their applications in natural language processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
