{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Modern AI Applications and Large Language Models\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand transformer architecture (lightly)\n",
    "- Learn about large language models (lightly)\n",
    "\n",
    "## Topics Covered:\n",
    "- Transformer architecture\n",
    "- Attention mechanisms\n",
    "- Large Language Models (GPT, BERT)\n",
    "- Natural Language Processing applications\n",
    "- Computer vision applications\n",
    "\n",
    "## Homework:\n",
    "Build-Your-Own Spotify Daylist\n",
    "\n",
    "## Case Study:\n",
    "Attention Is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    print(\"NLTK available\")\n",
    "except ImportError:\n",
    "    print(\"NLTK not available - install with: pip install nltk\")\n",
    "    nltk = None\n",
    "\n",
    "# Advanced NLP libraries\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "    print(\"Transformers library available\")\n",
    "except ImportError:\n",
    "    print(\"Transformers not available - install with: pip install transformers\")\n",
    "    pipeline = None\n",
    "\n",
    "# Deep Learning libraries\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available - install with: pip install tensorflow\")\n",
    "    tf = None\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if tf is not None:\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Modern AI and Natural Language Processing\n",
    "\n",
    "Modern AI has been revolutionized by advances in Natural Language Processing (NLP), particularly through the development of transformer architectures and large language models.\n",
    "\n",
    "### Evolution of NLP:\n",
    "1. **Traditional NLP** (1950s-2000s): Rule-based systems, bag-of-words\n",
    "2. **Statistical NLP** (2000s-2010s): Machine learning, feature engineering\n",
    "3. **Deep Learning NLP** (2010s): RNNs, LSTMs, word embeddings\n",
    "4. **Transformer Era** (2017-present): Attention mechanisms, BERT, GPT\n",
    "\n",
    "### Key Breakthroughs:\n",
    "- **Word Embeddings**: Word2Vec, GloVe (2013-2014)\n",
    "- **Attention Mechanisms**: Neural Machine Translation (2015)\n",
    "- **Transformer Architecture**: \"Attention Is All You Need\" (2017)\n",
    "- **BERT**: Bidirectional Encoder Representations (2018)\n",
    "- **GPT Series**: Generative Pre-trained Transformers (2018-present)\n",
    "\n",
    "### Modern Applications:\n",
    "- Machine translation\n",
    "- Text summarization\n",
    "- Question answering\n",
    "- Chatbots and virtual assistants\n",
    "- Content generation\n",
    "- Sentiment analysis\n",
    "- Code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample text data for demonstration\n",
    "sample_texts = [\n",
    "    \"I love this product! It's amazing and works perfectly.\",\n",
    "    \"This is terrible. Worst purchase ever. Complete waste of money.\",\n",
    "    \"The movie was okay. Not great, but not bad either.\",\n",
    "    \"Fantastic service! The staff was very helpful and friendly.\",\n",
    "    \"I hate waiting in long lines. This is so frustrating.\",\n",
    "    \"The weather is beautiful today. Perfect for a walk in the park.\",\n",
    "    \"This book is boring. I couldn't get past the first chapter.\",\n",
    "    \"Outstanding performance! The team did an excellent job.\",\n",
    "    \"The food was delicious. I'll definitely come back here.\",\n",
    "    \"Poor customer service. They were rude and unhelpful.\"\n",
    "]\n",
    "\n",
    "# Labels: 0 = negative, 1 = neutral, 2 = positive\n",
    "labels = [2, 0, 1, 2, 0, 2, 0, 2, 2, 0]\n",
    "label_names = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': sample_texts,\n",
    "    'label': labels,\n",
    "    'sentiment': [label_names[l] for l in labels]\n",
    "})\n",
    "\n",
    "print(\"Sample Text Data:\")\n",
    "print(df)\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "plt.bar(sentiment_counts.index, sentiment_counts.values, color=['red', 'gray', 'green'])\n",
    "plt.title('Sentiment Distribution in Sample Data')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention Mechanisms\n",
    "\n",
    "Attention mechanisms allow models to focus on relevant parts of the input when making predictions, similar to how humans selectively focus on important information.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Query (Q)**: What we're looking for\n",
    "- **Key (K)**: What we're looking at\n",
    "- **Value (V)**: The actual information\n",
    "- **Attention Weights**: How much to focus on each part\n",
    "- **Context Vector**: Weighted combination of values\n",
    "\n",
    "### Types of Attention:\n",
    "1. **Additive Attention**: Uses a neural network to compute attention scores\n",
    "2. **Multiplicative Attention**: Uses dot product for efficiency\n",
    "3. **Self-Attention**: Attention within the same sequence\n",
    "4. **Multi-Head Attention**: Multiple attention mechanisms in parallel\n",
    "\n",
    "### Mathematical Foundation:\n",
    "- **Attention Score**: score(Q, K) = Q · K^T\n",
    "- **Attention Weights**: α = softmax(score(Q, K))\n",
    "- **Context Vector**: C = Σ(α_i × V_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple attention mechanism demonstration\n",
    "def simple_attention_demo():\n",
    "    # Example sentence: \"The cat sat on the mat\"\n",
    "    words = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "    \n",
    "    # Simple word embeddings (random for demonstration)\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.randn(len(words), 4)  # 4-dimensional embeddings\n",
    "    \n",
    "    # Query: let's focus on the word \"cat\"\n",
    "    query_idx = 1  # \"cat\"\n",
    "    query = embeddings[query_idx]\n",
    "    \n",
    "    # Calculate attention scores (dot product)\n",
    "    scores = np.dot(embeddings, query)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
    "    \n",
    "    # Calculate context vector\n",
    "    context_vector = np.sum(attention_weights[:, np.newaxis] * embeddings, axis=0)\n",
    "    \n",
    "    print(\"=== ATTENTION MECHANISM DEMO ===\")\n",
    "    print(f\"Query word: {words[query_idx]}\")\n",
    "    print(f\"\\nAttention weights:\")\n",
    "    for i, (word, weight) in enumerate(zip(words, attention_weights)):\n",
    "        print(f\"{word}: {weight:.4f}\")\n",
    "    \n",
    "    # Visualize attention weights\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(words, attention_weights, color='skyblue')\n",
    "    bars[query_idx].set_color('orange')  # Highlight query word\n",
    "    plt.title(f'Attention Weights for Query: \"{words[query_idx]}\"')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Attention Weight')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return context_vector, attention_weights\n",
    "\n",
    "context, weights = simple_attention_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Architecture\n",
    "\n",
    "The Transformer architecture, introduced in \"Attention Is All You Need\" (2017), revolutionized NLP by relying entirely on attention mechanisms without recurrent or convolutional layers.\n",
    "\n",
    "### Key Components:\n",
    "1. **Multi-Head Attention**: Multiple attention mechanisms in parallel\n",
    "2. **Position Encoding**: Adds positional information to embeddings\n",
    "3. **Layer Normalization**: Stabilizes training\n",
    "4. **Feed-Forward Networks**: Point-wise transformations\n",
    "5. **Residual Connections**: Skip connections for gradient flow\n",
    "\n",
    "### Architecture:\n",
    "- **Encoder**: Processes input sequence\n",
    "- **Decoder**: Generates output sequence\n",
    "- **Self-Attention**: Relates positions within same sequence\n",
    "- **Cross-Attention**: Relates encoder and decoder\n",
    "\n",
    "### Advantages:\n",
    "- Parallelizable (unlike RNNs)\n",
    "- Captures long-range dependencies\n",
    "- More efficient training\n",
    "- Better performance on many tasks\n",
    "\n",
    "### Limitations:\n",
    "- Quadratic complexity with sequence length\n",
    "- Requires large amounts of data\n",
    "- Computationally expensive\n",
    "- Less interpretable than attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformer architecture components\n",
    "def visualize_transformer_concepts():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Multi-Head Attention concept\n",
    "    ax1 = axes[0, 0]\n",
    "    heads = ['Head 1', 'Head 2', 'Head 3', 'Head 4']\n",
    "    attention_patterns = np.random.rand(4, 6)  # 4 heads, 6 words\n",
    "    im1 = ax1.imshow(attention_patterns, cmap='Blues', aspect='auto')\n",
    "    ax1.set_title('Multi-Head Attention Pattern')\n",
    "    ax1.set_xlabel('Word Position')\n",
    "    ax1.set_ylabel('Attention Head')\n",
    "    ax1.set_yticks(range(4))\n",
    "    ax1.set_yticklabels(heads)\n",
    "    plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "    \n",
    "    # 2. Position Encoding\n",
    "    ax2 = axes[0, 1]\n",
    "    seq_len = 20\n",
    "    d_model = 8\n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                pos_encoding[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "    \n",
    "    im2 = ax2.imshow(pos_encoding.T, cmap='RdYlBu', aspect='auto')\n",
    "    ax2.set_title('Positional Encoding')\n",
    "    ax2.set_xlabel('Position')\n",
    "    ax2.set_ylabel('Embedding Dimension')\n",
    "    plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
    "    \n",
    "    # 3. Self-Attention Matrix\n",
    "    ax3 = axes[1, 0]\n",
    "    sentence = \"The quick brown fox jumps\".split()\n",
    "    n_words = len(sentence)\n",
    "    \n",
    "    # Simulate attention matrix\n",
    "    attention_matrix = np.random.rand(n_words, n_words)\n",
    "    # Make it more realistic (higher attention to nearby words)\n",
    "    for i in range(n_words):\n",
    "        for j in range(n_words):\n",
    "            distance = abs(i - j)\n",
    "            attention_matrix[i, j] *= np.exp(-distance * 0.3)\n",
    "    \n",
    "    # Normalize\n",
    "    attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    im3 = ax3.imshow(attention_matrix, cmap='Greens', aspect='auto')\n",
    "    ax3.set_title('Self-Attention Matrix')\n",
    "    ax3.set_xlabel('Key Words')\n",
    "    ax3.set_ylabel('Query Words')\n",
    "    ax3.set_xticks(range(n_words))\n",
    "    ax3.set_yticks(range(n_words))\n",
    "    ax3.set_xticklabels(sentence, rotation=45)\n",
    "    ax3.set_yticklabels(sentence)\n",
    "    plt.colorbar(im3, ax=ax3, shrink=0.8)\n",
    "    \n",
    "    # 4. Layer Structure\n",
    "    ax4 = axes[1, 1]\n",
    "    layers = ['Input\\nEmbedding', 'Multi-Head\\nAttention', 'Add & Norm', 'Feed\\nForward', 'Add & Norm', 'Output']\n",
    "    y_pos = np.arange(len(layers))\n",
    "    colors = ['lightblue', 'orange', 'lightgreen', 'orange', 'lightgreen', 'lightcoral']\n",
    "    \n",
    "    bars = ax4.barh(y_pos, [1]*len(layers), color=colors)\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels(layers)\n",
    "    ax4.set_xlabel('Layer Processing')\n",
    "    ax4.set_title('Transformer Layer Structure')\n",
    "    ax4.set_xlim(0, 1.2)\n",
    "    \n",
    "    # Add arrows\n",
    "    for i in range(len(layers)-1):\n",
    "        ax4.annotate('', xy=(0.5, i+1), xytext=(0.5, i),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_transformer_concepts()\n",
    "\n",
    "print(\"\\nTransformer Key Innovations:\")\n",
    "print(\"1. Multi-Head Attention: Multiple attention mechanisms capture different relationships\")\n",
    "print(\"2. Positional Encoding: Sine/cosine functions encode position information\")\n",
    "print(\"3. Self-Attention: Each word attends to all other words in the sequence\")\n",
    "print(\"4. Layer Normalization: Stabilizes training and enables deeper networks\")\n",
    "print(\"5. Residual Connections: Skip connections help with gradient flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Large Language Models (LLMs)\n",
    "\n",
    "Large Language Models are transformer-based models trained on vast amounts of text data to understand and generate human-like text.\n",
    "\n",
    "### Key Models:\n",
    "1. **BERT** (2018): Bidirectional Encoder Representations from Transformers\n",
    "2. **GPT Series** (2018-2023): Generative Pre-trained Transformers\n",
    "3. **T5** (2019): Text-to-Text Transfer Transformer\n",
    "4. **PaLM** (2022): Pathways Language Model\n",
    "5. **ChatGPT/GPT-4** (2022-2023): Conversational AI systems\n",
    "\n",
    "### Training Process:\n",
    "1. **Pre-training**: Large-scale unsupervised learning on text corpora\n",
    "2. **Fine-tuning**: Task-specific supervised learning\n",
    "3. **Instruction Tuning**: Training to follow instructions\n",
    "4. **Reinforcement Learning from Human Feedback (RLHF)**: Alignment with human preferences\n",
    "\n",
    "### Capabilities:\n",
    "- Text generation and completion\n",
    "- Question answering\n",
    "- Summarization\n",
    "- Translation\n",
    "- Code generation\n",
    "- Reasoning and problem-solving\n",
    "- Creative writing\n",
    "\n",
    "### Challenges:\n",
    "- Computational requirements\n",
    "- Hallucination (generating false information)\n",
    "- Bias and fairness\n",
    "- Interpretability\n",
    "- Safety and alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate basic NLP tasks that LLMs excel at\n",
    "def demonstrate_nlp_tasks():\n",
    "    print(\"=== LARGE LANGUAGE MODEL CAPABILITIES ===\")\n",
    "    \n",
    "    # Sample texts for different tasks\n",
    "    tasks = {\n",
    "        'Text Classification': {\n",
    "            'description': 'Classify text into categories (sentiment, topic, etc.)',\n",
    "            'example': 'The new iPhone is amazing! I love the camera quality.',\n",
    "            'output': 'Sentiment: Positive, Topic: Technology'\n",
    "        },\n",
    "        'Text Summarization': {\n",
    "            'description': 'Create concise summaries of longer texts',\n",
    "            'example': 'Climate change is a long-term shift in global temperatures and weather patterns. While climate change is natural, human activities have been the main driver since the 1800s...',\n",
    "            'output': 'Summary: Climate change refers to long-term temperature and weather shifts, primarily caused by human activities since the 1800s.'\n",
    "        },\n",
    "        'Question Answering': {\n",
    "            'description': 'Answer questions based on context or knowledge',\n",
    "            'example': 'Context: Paris is the capital of France. Question: What is the capital of France?',\n",
    "            'output': 'Answer: Paris'\n",
    "        },\n",
    "        'Text Generation': {\n",
    "            'description': 'Generate coherent text based on prompts',\n",
    "            'example': 'Write a story about a robot who learns to paint...',\n",
    "            'output': 'Generated: In a small workshop, R2-D7 discovered brushes and colors. Day by day, it learned to express emotions through art...'\n",
    "        },\n",
    "        'Translation': {\n",
    "            'description': 'Translate text between languages',\n",
    "            'example': 'English: Hello, how are you? → Spanish: Hola, ¿cómo estás?',\n",
    "            'output': 'Translation successful with high accuracy'\n",
    "        },\n",
    "        'Code Generation': {\n",
    "            'description': 'Generate code from natural language descriptions',\n",
    "            'example': 'Create a Python function that calculates the factorial of a number',\n",
    "            'output': 'def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for task_name, task_info in tasks.items():\n",
    "        print(f\"\\n{task_name}:\")\n",
    "        print(f\"  Description: {task_info['description']}\")\n",
    "        print(f\"  Example Input: {task_info['example'][:100]}...\" if len(task_info['example']) > 100 else f\"  Example Input: {task_info['example']}\")\n",
    "        print(f\"  Expected Output: {task_info['output']}\")\n",
    "    \n",
    "    # Visualize model scale evolution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    models = ['BERT-Base', 'GPT-1', 'GPT-2', 'GPT-3', 'PaLM', 'GPT-4']\n",
    "    parameters = [110, 117, 1500, 175000, 540000, 1000000]  # in millions\n",
    "    years = [2018, 2018, 2019, 2020, 2022, 2023]\n",
    "    \n",
    "    colors = ['blue', 'green', 'orange', 'red', 'purple', 'brown']\n",
    "    \n",
    "    plt.scatter(years, parameters, c=colors, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, (model, param, year) in enumerate(zip(models, parameters, years)):\n",
    "        plt.annotate(f'{model}\\n{param}M params', \n",
    "                    (year, param), \n",
    "                    xytext=(5, 5), \n",
    "                    textcoords='offset points',\n",
    "                    fontsize=9,\n",
    "                    ha='left')\n",
    "    \n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Parameters (millions)')\n",
    "    plt.title('Evolution of Large Language Models')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_nlp_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BERT: Bidirectional Encoder Representations\n",
    "\n",
    "BERT revolutionized NLP by introducing bidirectional training, allowing the model to understand context from both directions.\n",
    "\n",
    "### Key Features:\n",
    "- **Bidirectional**: Reads text in both directions\n",
    "- **Encoder-only**: Uses only the encoder part of transformer\n",
    "- **Pre-training Tasks**: Masked Language Modeling, Next Sentence Prediction\n",
    "- **Fine-tuning**: Adapts to specific tasks\n",
    "\n",
    "### Architecture:\n",
    "- **BERT-Base**: 12 layers, 768 hidden units, 12 attention heads\n",
    "- **BERT-Large**: 24 layers, 1024 hidden units, 16 attention heads\n",
    "- **WordPiece Tokenization**: Subword units\n",
    "- **Special Tokens**: [CLS], [SEP], [MASK]\n",
    "\n",
    "### Applications:\n",
    "- Text classification\n",
    "- Named entity recognition\n",
    "- Question answering\n",
    "- Sentiment analysis\n",
    "- Text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate BERT concepts (without actual BERT model)\n",
    "def demonstrate_bert_concepts():\n",
    "    print(\"=== BERT CONCEPTS DEMONSTRATION ===\")\n",
    "    \n",
    "    # Demonstrate masked language modeling concept\n",
    "    original_sentence = \"The cat sat on the mat\"\n",
    "    masked_sentence = \"The cat [MASK] on the mat\"\n",
    "    \n",
    "    print(\"\\n1. Masked Language Modeling:\")\n",
    "    print(f\"Original: {original_sentence}\")\n",
    "    print(f\"Masked: {masked_sentence}\")\n",
    "    print(f\"BERT predicts: 'sat' (based on bidirectional context)\")\n",
    "    \n",
    "    # Demonstrate next sentence prediction\n",
    "    sentence_a = \"I went to the store.\"\n",
    "    sentence_b_correct = \"I bought some milk.\"\n",
    "    sentence_b_incorrect = \"The weather is nice today.\"\n",
    "    \n",
    "    print(\"\\n2. Next Sentence Prediction:\")\n",
    "    print(f\"Sentence A: {sentence_a}\")\n",
    "    print(f\"Sentence B (correct): {sentence_b_correct}\")\n",
"    print(f\"Sentence B (incorrect): {sentence_b_incorrect}\")\n",
"    print(f\"BERT predicts: Sentence B is a valid continuation (IsNext=True)\")\n",
"    print(f\"BERT predicts: Sentence B is NOT a valid continuation (IsNext=False)\")\n",
"    \n",
"    # Demonstrate tokenization\n",
"    sample_text = \"Hello, world! This is BERT.\"\n",
"    print(f\"\\n3. Tokenization (WordPiece):\")\n",
"    print(f\"Original: {sample_text}\")\n",
"    print(f\"Tokens: ['[CLS]', 'Hello', ',', 'world', '!', 'This', 'is', 'BE', '##RT', '.', '[SEP]']\")\n",
"    print(f\"Note: '##RT' indicates a subword continuation\")\n",
"    \n",
"    # Visualize BERT architecture\n",
"    plt.figure(figsize=(12, 8))\n",
"    \n",
"    # BERT vs GPT comparison\n",
"    models = ['BERT-Base', 'BERT-Large', 'GPT-1', 'GPT-2', 'GPT-3']\n",
"    params = [110, 340, 117, 1500, 175000]\n",
"    layers = [12, 24, 12, 48, 96]\n",
"    \n",
"    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
"    \n",
"    # Parameters comparison\n",
"    colors = ['blue', 'darkblue', 'green', 'orange', 'red']\n",
"    ax1.bar(models, params, color=colors, alpha=0.7)\n",
"    ax1.set_yscale('log')\n",
"    ax1.set_ylabel('Parameters (millions)')\n",
"    ax1.set_title('Model Size Comparison')\n",
"    ax1.tick_params(axis='x', rotation=45)\n",
"    \n",
"    # Layers comparison\n",
"    ax2.bar(models, layers, color=colors, alpha=0.7)\n",
"    ax2.set_ylabel('Number of Layers')\n",
"    ax2.set_title('Model Depth Comparison')\n",
"    ax2.tick_params(axis='x', rotation=45)\n",
"    \n",
"    plt.tight_layout()\n",
"    plt.show()\n",
"\n",
"demonstrate_bert_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GPT: Generative Pre-trained Transformers\n",
    "\n",
    "The GPT series represents the generative approach to language modeling, focusing on text generation rather than understanding.\n",
    "\n",
    "### Key Features:\n",
    "- **Autoregressive**: Predicts next token based on previous tokens\n",
    "- **Decoder-only**: Uses only the decoder part of transformer\n",
    "- **Causal Attention**: Can only attend to previous tokens\n",
    "- **Zero-shot/Few-shot Learning**: Performs tasks without specific training\n",
    "\n",
    "### Evolution:\n",
    "- **GPT-1** (2018): 117M parameters, proof of concept\n",
    "- **GPT-2** (2019): 1.5B parameters, impressive text generation\n",
    "- **GPT-3** (2020): 175B parameters, few-shot learning breakthrough\n",
    "- **GPT-4** (2023): Multimodal capabilities, improved reasoning\n",
    "\n",
    "### Applications:\n",
    "- Text generation and completion\n",
    "- Creative writing\n",
    "- Code generation\n",
    "- Conversational AI\n",
    "- Content creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate GPT concepts and capabilities\n",
    "def demonstrate_gpt_concepts():\n",
    "    print(\"=== GPT CONCEPTS DEMONSTRATION ===\")\n",
    "    \n",
    "    # Demonstrate autoregressive generation\n",
    "    prompt = \"The future of artificial intelligence is\"\n",
    "    tokens = prompt.split()\n",
    "    \n",
    "    print(\"\\n1. Autoregressive Text Generation:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"Generation process:\")\n",
    "    \n",
    "    # Simulate token-by-token generation\n",
    "    generated_tokens = [\"bright\", \"because\", \"it\", \"will\", \"help\", \"solve\", \"complex\", \"problems\"]\n",
    "    current_text = prompt\n",
    "    \n",
    "    for i, token in enumerate(generated_tokens):\n",
    "        print(f\"  Step {i+1}: '{current_text}' → '{token}'\")\n",
    "        current_text += \" \" + token\n",
    "    \n",
    "    print(f\"\\nFinal output: {current_text}\")\n",
    "    \n",
    "    # Demonstrate few-shot learning\n",
    "    print(\"\\n2. Few-shot Learning Example:\")\n",
    "    few_shot_prompt = '''Examples:\n",
    "Input: \"I love this movie!\"\n",
    "Output: Positive\n",
    "\n",
    "Input: \"This is terrible.\"\n",
    "Output: Negative\n",
    "\n",
    "Input: \"The weather is okay.\"\n",
    "Output: Neutral\n",
    "\n",
    "Input: \"This restaurant is amazing!\"\n",
    "Output: '''\n",
    "    \n",
    "    print(few_shot_prompt)\n",
    "    print(\"Expected: Positive\")\n",
    "    \n",
    "    # Compare BERT vs GPT\n",
    "    print(\"\\n3. BERT vs GPT Comparison:\")\n",
    "    comparison = {\n",
    "        'Architecture': ['Encoder-only', 'Decoder-only'],\n",
    "        'Training': ['Bidirectional', 'Autoregressive'],\n",
    "        'Best for': ['Understanding', 'Generation'],\n",
    "        'Attention': ['Bidirectional', 'Causal (masked)'],\n",
    "        'Applications': ['Classification, QA', 'Generation, Chat']\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Aspect':<15} {'BERT':<20} {'GPT':<20}\")\n",
    "    print(\"-\" * 55)\n",
    "    for aspect, (bert_val, gpt_val) in comparison.items():\n",
    "        print(f\"{aspect:<15} {bert_val:<20} {gpt_val:<20}\")\n",
    "\n",
    "demonstrate_gpt_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build-Your-Own Spotify Daylist (Homework Project)\n",
    "\n",
    "For this week's homework, you'll create a simple recommendation system inspired by Spotify's Daylist feature, which creates personalized playlists based on time of day and user preferences.\n",
    "\n",
    "### Project Overview:\n",
    "Create a system that:\n",
    "1. Analyzes user listening patterns by time of day\n",
    "2. Categorizes music by mood/energy level\n",
    "3. Generates time-appropriate playlists\n",
    "4. Uses basic NLP for mood detection from song lyrics/titles\n",
    "\n",
    "### Components:\n",
    "- **Data**: Sample music dataset with features\n",
    "- **Time Analysis**: Listening patterns by hour\n",
    "- **Mood Classification**: Energy, valence, danceability\n",
    "- **Recommendation Engine**: Content-based filtering\n",
    "- **Playlist Generation**: Time-aware recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample implementation of Spotify Daylist concept\n",
    "def create_sample_daylist_system():\n",
    "    print(\"=== BUILD-YOUR-OWN SPOTIFY DAYLIST ===\")\n",
    "    \n",
    "    # Sample music data\n",
    "    music_data = {\n",
    "        'song_id': range(1, 21),\n",
    "        'title': [\n",
    "            'Morning Sunshine', 'Coffee Blues', 'Energetic Workout', 'Smooth Jazz',\n",
    "            'Upbeat Pop', 'Chill Vibes', 'Rock Anthem', 'Acoustic Calm',\n",
    "            'Electronic Dance', 'Mellow Evening', 'Late Night Ballad', 'Sunrise Melody',\n",
    "            'Power Workout', 'Relaxing Piano', 'Party Time', 'Quiet Reflection',\n",
    "            'Morning Motivation', 'Afternoon Delight', 'Evening Jazz', 'Nighttime Lullaby'\n",
    "        ],\n",
    "        'energy': [0.8, 0.3, 0.9, 0.2, 0.7, 0.4, 0.8, 0.3, 0.9, 0.2, 0.1, 0.6, 0.9, 0.2, 0.8, 0.3, 0.7, 0.5, 0.3, 0.1],\n",
    "        'valence': [0.9, 0.4, 0.8, 0.6, 0.8, 0.7, 0.7, 0.8, 0.9, 0.6, 0.3, 0.8, 0.9, 0.9, 0.9, 0.5, 0.8, 0.7, 0.6, 0.4],\n",
    "        'danceability': [0.6, 0.2, 0.8, 0.3, 0.7, 0.5, 0.6, 0.2, 0.9, 0.3, 0.2, 0.5, 0.8, 0.1, 0.9, 0.2, 0.6, 0.6, 0.4, 0.1],\n",
    "        'optimal_hour': [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 6, 19, 23]\n",
    "    }\n",
    "    \n",
    "    df_music = pd.DataFrame(music_data)\n",
    "    print(\"Sample Music Dataset:\")\n",
    "    print(df_music.head(10))\n",
    "    \n",
    "    # Define time periods and their characteristics\n",
    "    time_periods = {\n",
    "        'Early Morning (6-8)': {'energy': 0.6, 'valence': 0.7, 'danceability': 0.4},\n",
    "        'Morning (9-11)': {'energy': 0.7, 'valence': 0.8, 'danceability': 0.6},\n",
    "        'Afternoon (12-17)': {'energy': 0.6, 'valence': 0.7, 'danceability': 0.5},\n",
    "        'Evening (18-21)': {'energy': 0.5, 'valence': 0.6, 'danceability': 0.4},\n",
    "        'Night (22-23)': {'energy': 0.3, 'valence': 0.5, 'danceability': 0.2}\n",
    "    }\n",
    "    \n",
    "    # Generate daylist for different times\n",
    "    def generate_daylist(hour, num_songs=5):\n",
    "        # Determine time period\n",
    "        if 6 <= hour <= 8:\n",
    "            period = 'Early Morning (6-8)'\n",
    "        elif 9 <= hour <= 11:\n",
    "            period = 'Morning (9-11)'\n",
    "        elif 12 <= hour <= 17:\n",
    "            period = 'Afternoon (12-17)'\n",
    "        elif 18 <= hour <= 21:\n",
    "            period = 'Evening (18-21)'\n",
    "        else:\n",
    "            period = 'Night (22-23)'\n",
    "        \n",
    "        target_features = time_periods[period]\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        df_music['similarity'] = (\n",
    "            abs(df_music['energy'] - target_features['energy']) * 0.4 +\n",
    "            abs(df_music['valence'] - target_features['valence']) * 0.3 +\n",
    "            abs(df_music['danceability'] - target_features['danceability']) * 0.3\n",
    "        )\n",
    "        \n",
    "        # Get top recommendations\n",
    "        recommendations = df_music.nsmallest(num_songs, 'similarity')\n",
    "        \n",
    "        return period, recommendations[['title', 'energy', 'valence', 'danceability']]\n",
    "    \n",
    "    # Generate playlists for different times\n",
    "    test_hours = [7, 10, 15, 20, 23]\n",
    "    \n",
    "    for hour in test_hours:\n",
    "        period, playlist = generate_daylist(hour)\n",
    "        print(f\"\\n{hour}:00 - {period} Daylist:\")\n",
    "        print(playlist.to_string(index=False))\n",
    "    \n",
    "    # Visualize music features by time\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Energy by optimal hour\n",
    "    axes[0, 0].scatter(df_music['optimal_hour'], df_music['energy'], alpha=0.6, color='red')\n",
    "    axes[0, 0].set_xlabel('Hour of Day')\n",
    "    axes[0, 0].set_ylabel('Energy Level')\n",
    "    axes[0, 0].set_title('Energy vs Time of Day')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Valence by optimal hour\n",
    "    axes[0, 1].scatter(df_music['optimal_hour'], df_music['valence'], alpha=0.6, color='blue')\n",
    "    axes[0, 1].set_xlabel('Hour of Day')\n",
    "    axes[0, 1].set_ylabel('Valence (Positivity)')\n",
    "    axes[0, 1].set_title('Valence vs Time of Day')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Danceability by optimal hour\n",
    "    axes[1, 0].scatter(df_music['optimal_hour'], df_music['danceability'], alpha=0.6, color='green')\n",
    "    axes[1, 0].set_xlabel('Hour of Day')\n",
    "    axes[1, 0].set_ylabel('Danceability')\n",
    "    axes[1, 0].set_title('Danceability vs Time of Day')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature correlation heatmap\n",
    "    correlation_matrix = df_music[['energy', 'valence', 'danceability']].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Feature Correlation Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_music\n",
    "\n",
    "sample_data = create_sample_daylist_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Congratulations! You've completed your introduction to modern AI applications and large language models. Here's what you learned:\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "1. **Attention Mechanisms**: Query, Key, Value paradigm and self-attention\n",
    "2. **Transformer Architecture**: Multi-head attention, positional encoding, layer normalization\n",
    "3. **Large Language Models**: BERT, GPT series, and their capabilities\n",
    "4. **Pre-training and Fine-tuning**: Modern AI training paradigms\n",
    "5. **NLP Applications**: Text classification, generation, translation, summarization\n",
    "6. **Recommendation Systems**: Content-based filtering and personalization\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- Understanding modern AI architectures\n",
    "- Recognizing when to use different model types\n",
    "- Building simple recommendation systems\n",
    "- Analyzing text data and user preferences\n",
    "- Implementing basic NLP pipelines\n",
    "- Creating time-aware applications\n",
    "\n",
    "### Revolution in AI:\n",
    "- **\"Attention Is All You Need\"** (2017) changed everything\n",
    "- Transformers enabled large-scale language models\n",
    "- BERT showed the power of bidirectional understanding\n",
    "- GPT demonstrated impressive text generation capabilities\n",
    "- Modern LLMs can perform many tasks with minimal training\n",
    "\n",
    "### Real-world Applications:\n",
    "- **Search Engines**: Better understanding of user queries\n",
    "- **Virtual Assistants**: More natural conversations\n",
    "- **Content Creation**: Automated writing and editing\n",
    "- **Customer Service**: Intelligent chatbots\n",
    "- **Education**: Personalized tutoring systems\n",
    "- **Healthcare**: Medical text analysis\n",
    "- **Finance**: Document processing and analysis\n",
    "\n",
    "### Current Challenges:\n",
    "- **Computational Cost**: Training and inference are expensive\n",
    "- **Data Requirements**: Need massive amounts of training data\n",
    "- **Bias and Fairness**: Models can perpetuate societal biases\n",
    "- **Interpretability**: Difficult to understand model decisions\n",
    "- **Hallucination**: Models can generate false information\n",
    "- **Safety**: Ensuring AI systems are aligned with human values\n",
    "\n",
    "### Future Directions:\n",
    "- **Multimodal Models**: Combining text, images, and audio\n",
    "- **Efficient Architectures**: Reducing computational requirements\n",
    "- **Better Alignment**: Making AI systems more helpful and harmless\n",
    "- **Specialized Applications**: Domain-specific AI assistants\n",
    "- **Human-AI Collaboration**: Augmenting human capabilities\n",
    "\n",
    "### Homework Project:\n",
    "Your \"Build-Your-Own Spotify Daylist\" project demonstrates how modern AI concepts can be applied to create personalized, time-aware recommendations. This combines:\n",
    "- Content analysis (music features)\n",
    "- User behavior modeling (time preferences)\n",
    "- Recommendation algorithms (similarity matching)\n",
    "- Personalization (individual preferences)\n",
    "\n",
    "This project showcases how AI can enhance user experiences by understanding context and preferences, similar to how large language models adapt their responses based on context and user needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
