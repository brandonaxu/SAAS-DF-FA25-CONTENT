{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Multiple Linear Regression and Fine Tuning\n",
    "\n",
    "## Learning Objectives:\n",
    "- Be able to conceptually understand how linear regression works as more features are added\n",
    "- Understand how Lasso, Ridge, and ElasticNet change the Linear Regression model\n",
    "- Be able to make decisions about how to choose and fine tune their model\n",
    "\n",
    "## Topics Covered:\n",
    "- Multiple Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression extends simple linear regression by using multiple features to predict a target variable. As we add more features, the model becomes more flexible but also more prone to overfitting.\n",
    "\n",
    "### Mathematical Foundation:\n",
    "```\n",
    "y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε\n",
    "```\n",
    "\n",
    "Where:\n",
    "- y = target variable\n",
    "- β₀ = intercept\n",
    "- β₁, β₂, ..., βₙ = coefficients for each feature\n",
    "- x₁, x₂, ..., xₙ = features\n",
    "- ε = error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dataset with multiple features\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Generate synthetic house price data with many features\n",
    "house_size = np.random.normal(2000, 600, n_samples)\n",
    "bedrooms = np.random.randint(1, 6, n_samples)\n",
    "bathrooms = np.random.randint(1, 4, n_samples)\n",
    "age = np.random.randint(0, 50, n_samples)\n",
    "lot_size = np.random.normal(8000, 2000, n_samples)\n",
    "garage = np.random.randint(0, 4, n_samples)\n",
    "neighborhood_score = np.random.normal(7, 2, n_samples)  # 1-10 scale\n",
    "distance_to_city = np.random.normal(15, 8, n_samples)  # miles\n",
    "\n",
    "# Create realistic relationships for house prices\n",
    "price = (house_size * 100 + \n",
    "         bedrooms * 10000 + \n",
    "         bathrooms * 15000 + \n",
    "         -age * 800 +\n",
    "         lot_size * 5 +\n",
    "         garage * 8000 +\n",
    "         neighborhood_score * 12000 +\n",
    "         -distance_to_city * 2000 +\n",
    "         np.random.normal(0, 25000, n_samples))\n",
    "\n",
    "# Create DataFrame\n",
    "house_data = pd.DataFrame({\n",
    "    'House_Size': house_size,\n",
    "    'Bedrooms': bedrooms,\n",
    "    'Bathrooms': bathrooms,\n",
    "    'Age': age,\n",
    "    'Lot_Size': lot_size,\n",
    "    'Garage': garage,\n",
    "    'Neighborhood_Score': neighborhood_score,\n",
    "    'Distance_to_City': distance_to_city,\n",
    "    'Price': price\n",
    "})\n",
    "\n",
    "print(\"House Price Dataset:\")\n",
    "print(house_data.head())\n",
    "print(f\"\\nDataset shape: {house_data.shape}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(house_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline multiple linear regression model\n",
    "print(\"=== BASELINE MULTIPLE LINEAR REGRESSION ===\")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = ['House_Size', 'Bedrooms', 'Bathrooms', 'Age', 'Lot_Size', \n",
    "                  'Garage', 'Neighborhood_Score', 'Distance_to_City']\n",
    "X = house_data[feature_columns]\n",
    "y = house_data['Price']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = lr_model.predict(X_train_scaled)\n",
    "y_pred_test = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training RMSE: ${train_rmse:,.2f}\")\n",
    "print(f\"Test RMSE: ${test_rmse:,.2f}\")\n",
    "print(f\"Training R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "\n",
    "# Display coefficients\n",
    "print(f\"\\nModel coefficients:\")\n",
    "print(f\"Intercept: ${lr_model.intercept_:,.2f}\")\n",
    "for feature, coef in zip(feature_columns, lr_model.coef_):\n",
    "    print(f\"{feature:<20}: {coef:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge Regression (L2 Regularization)\n",
    "\n",
    "Ridge regression adds a penalty term to the linear regression cost function to prevent overfitting. It shrinks the coefficients towards zero but doesn't eliminate them entirely.\n",
    "\n",
    "### Mathematical Foundation:\n",
    "```\n",
    "Cost = MSE + α * Σ(βᵢ²)\n",
    "```\n",
    "\n",
    "Where α (alpha) is the regularization parameter that controls the strength of the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression Implementation\n",
    "print(\"=== RIDGE REGRESSION (L2 REGULARIZATION) ===\")\n",
    "\n",
    "# Try different alpha values\n",
    "alphas = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "ridge_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Create Ridge model\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train_ridge = ridge_model.predict(X_train_scaled)\n",
    "    y_pred_test_ridge = ridge_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_ridge))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_ridge))\n",
    "    train_r2 = r2_score(y_train, y_pred_train_ridge)\n",
    "    test_r2 = r2_score(y_test, y_pred_test_ridge)\n",
    "    \n",
    "    ridge_results.append({\n",
    "        'alpha': alpha,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'coefficients': ridge_model.coef_.copy()\n",
    "    })\n",
    "    \n",
    "    print(f\"Alpha {alpha:6.2f}: Train RMSE: ${train_rmse:8,.0f}, Test RMSE: ${test_rmse:8,.0f}, \"\n",
    "          f\"Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "ridge_df = pd.DataFrame(ridge_results)\n",
    "\n",
    "# Find best alpha\n",
    "best_alpha_idx = ridge_df['test_r2'].idxmax()\n",
    "best_alpha = ridge_df.loc[best_alpha_idx, 'alpha']\n",
    "print(f\"\\nBest alpha based on test R²: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso regression also adds a penalty term, but it uses the absolute value of coefficients. This can drive some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "### Mathematical Foundation:\n",
    "```\n",
    "Cost = MSE + α * Σ|βᵢ|\n",
    "```\n",
    "\n",
    "Where α (alpha) is the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression Implementation\n",
    "print(\"=== LASSO REGRESSION (L1 REGULARIZATION) ===\")\n",
    "\n",
    "# Try different alpha values\n",
    "lasso_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Create Lasso model\n",
    "    lasso_model = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train_lasso = lasso_model.predict(X_train_scaled)\n",
    "    y_pred_test_lasso = lasso_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_lasso))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_lasso))\n",
    "    train_r2 = r2_score(y_train, y_pred_train_lasso)\n",
    "    test_r2 = r2_score(y_test, y_pred_test_lasso)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    n_nonzero = np.sum(lasso_model.coef_ != 0)\n",
    "    \n",
    "    lasso_results.append({\n",
    "        'alpha': alpha,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'n_nonzero': n_nonzero,\n",
    "        'coefficients': lasso_model.coef_.copy()\n",
    "    })\n",
    "    \n",
    "    print(f\"Alpha {alpha:6.2f}: Train RMSE: ${train_rmse:8,.0f}, Test RMSE: ${test_rmse:8,.0f}, \"\n",
    "          f\"Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}, Non-zero coefs: {n_nonzero}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "lasso_df = pd.DataFrame(lasso_results)\n",
    "\n",
    "# Find best alpha\n",
    "best_lasso_idx = lasso_df['test_r2'].idxmax()\n",
    "best_lasso_alpha = lasso_df.loc[best_lasso_idx, 'alpha']\n",
    "print(f\"\\nBest alpha based on test R²: {best_lasso_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ElasticNet Regression\n",
    "\n",
    "ElasticNet combines both L1 (Lasso) and L2 (Ridge) regularization. It provides a balance between feature selection and coefficient shrinkage.\n",
    "\n",
    "### Mathematical Foundation:\n",
    "```\n",
    "Cost = MSE + α * [l1_ratio * Σ|βᵢ| + (1 - l1_ratio) * Σ(βᵢ²)]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- α = regularization strength\n",
    "- l1_ratio = mixing parameter (0 = Ridge, 1 = Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet Regression Implementation\n",
    "print(\"=== ELASTICNET REGRESSION ===\")\n",
    "\n",
    "# Try different l1_ratio values\n",
    "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "elastic_results = []\n",
    "\n",
    "for l1_ratio in l1_ratios:\n",
    "    # Use alpha=1.0 for comparison\n",
    "    elastic_model = ElasticNet(alpha=1.0, l1_ratio=l1_ratio, max_iter=10000)\n",
    "    elastic_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train_elastic = elastic_model.predict(X_train_scaled)\n",
    "    y_pred_test_elastic = elastic_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_elastic))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_elastic))\n",
    "    train_r2 = r2_score(y_train, y_pred_train_elastic)\n",
    "    test_r2 = r2_score(y_test, y_pred_test_elastic)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    n_nonzero = np.sum(elastic_model.coef_ != 0)\n",
    "    \n",
    "    elastic_results.append({\n",
    "        'l1_ratio': l1_ratio,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'n_nonzero': n_nonzero,\n",
    "        'coefficients': elastic_model.coef_.copy()\n",
    "    })\n",
    "    \n",
    "    print(f\"L1_ratio {l1_ratio:.1f}: Train RMSE: ${train_rmse:8,.0f}, Test RMSE: ${test_rmse:8,.0f}, \"\n",
    "          f\"Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}, Non-zero coefs: {n_nonzero}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "elastic_df = pd.DataFrame(elastic_results)\n",
    "\n",
    "# Find best l1_ratio\n",
    "best_elastic_idx = elastic_df['test_r2'].idxmax()\n",
    "best_l1_ratio = elastic_df.loc[best_elastic_idx, 'l1_ratio']\n",
    "print(f\"\\nBest l1_ratio based on test R²: {best_l1_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Selection\n",
    "\n",
    "Let's compare all the models and use cross-validation to select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model comparison\n",
    "print(\"=== COMPREHENSIVE MODEL COMPARISON ===\")\n",
    "\n",
    "# Define models with their best parameters\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=best_alpha),\n",
    "    'Lasso': Lasso(alpha=best_lasso_alpha, max_iter=10000),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=best_l1_ratio, max_iter=10000)\n",
    "}\n",
    "\n",
    "# Cross-validation comparison\n",
    "cv_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    cv_rmse_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse_scores = np.sqrt(-cv_rmse_scores)\n",
    "    \n",
    "    # Train on full training set and test\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'CV_R2_Mean': cv_scores.mean(),\n",
    "        'CV_R2_Std': cv_scores.std(),\n",
    "        'CV_RMSE_Mean': cv_rmse_scores.mean(),\n",
    "        'CV_RMSE_Std': cv_rmse_scores.std(),\n",
    "        'Test_R2': test_r2,\n",
    "        'Test_RMSE': test_rmse\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(cv_results_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = cv_results_df['Test_R2'].idxmax()\n",
    "best_model_name = cv_results_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nBest model based on test R²: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Let's use GridSearchCV to find the optimal hyperparameters for each regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for best hyperparameters\n",
    "print(\"=== GRID SEARCH FOR OPTIMAL HYPERPARAMETERS ===\")\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search_results = {}\n",
    "\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    print(f\"\\nGrid searching {model_name}...\")\n",
    "    \n",
    "    if model_name == 'Ridge':\n",
    "        model = Ridge()\n",
    "    elif model_name == 'Lasso':\n",
    "        model = Lasso(max_iter=10000)\n",
    "    elif model_name == 'ElasticNet':\n",
    "        model = ElasticNet(max_iter=10000)\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Test the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_test = best_model.predict(X_test_scaled)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    grid_search_results[model_name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'best_model': best_model\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV R²: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test R²: {test_r2:.4f}\")\n",
    "    print(f\"Test RMSE: ${test_rmse:,.2f}\")\n",
    "\n",
    "# Find overall best model\n",
    "best_overall = max(grid_search_results.items(), key=lambda x: x[1]['test_r2'])\n",
    "print(f\"\\nOverall best model: {best_overall[0]}\")\n",
    "print(f\"Parameters: {best_overall[1]['best_params']}\")\n",
    "print(f\"Test R²: {best_overall[1]['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "Congratulations! You've completed your deep dive into multiple linear regression and regularization techniques. Here's what you learned:\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "1. **Multiple Linear Regression**: Using multiple features to predict outcomes\n",
    "2. **Ridge Regression (L2)**: Shrinks coefficients to prevent overfitting\n",
    "3. **Lasso Regression (L1)**: Performs feature selection by driving coefficients to zero\n",
    "4. **ElasticNet**: Combines Ridge and Lasso for balanced regularization\n",
    "5. **Hyperparameter Tuning**: Using GridSearchCV to find optimal parameters\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- Understanding when and why to use regularization\n",
    "- Implementing Ridge, Lasso, and ElasticNet regression\n",
    "- Comparing model performance across different regularization techniques\n",
    "- Using cross-validation for model selection\n",
    "- Making informed decisions about model complexity\n",
    "\n",
    "### When to Use Each Technique:\n",
    "- **Linear Regression**: When you have few features and no overfitting\n",
    "- **Ridge Regression**: When you want to keep all features but prevent overfitting\n",
    "- **Lasso Regression**: When you want automatic feature selection\n",
    "- **ElasticNet**: When you want both regularization and some feature selection\n",
    "\n",
    "### Best Practices to Remember:\n",
    "- Always scale your features before applying regularization\n",
    "- Use cross-validation to select hyperparameters\n",
    "- Start with simple models before adding complexity\n",
    "- Consider the interpretability vs accuracy trade-off\n",
    "- Regularization helps prevent overfitting, especially with limited data\n",
    "\n",
    "### Next Steps:\n",
    "In the next week, we'll explore classification algorithms including logistic regression, decision trees, and random forests. You'll learn how to apply these concepts to classification problems and understand different evaluation metrics for classification models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
