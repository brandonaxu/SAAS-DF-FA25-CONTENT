{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Classification Algorithms\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand classification problems\n",
    "- Implement various classification algorithms\n",
    "- Master classification evaluation metrics\n",
    "- Understand how certain parameters change the behavior of common classification models\n",
    "\n",
    "## Topics Covered:\n",
    "- Logistic regression\n",
    "- Decision trees\n",
    "- Random forests\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Classification metrics (accuracy, precision, recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, classification_report, confusion_matrix,\n",
    "                           roc_auc_score, roc_curve)\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Classification Problems\n",
    "\n",
    "Classification is a supervised learning task where we predict discrete categories or classes. Unlike regression (predicting continuous values), classification predicts categorical outcomes.\n",
    "\n",
    "### Types of Classification:\n",
    "- **Binary Classification**: Two classes (e.g., spam vs. not spam)\n",
    "- **Multi-class Classification**: More than two classes (e.g., flower species)\n",
    "- **Multi-label Classification**: Multiple labels per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive classification dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic customer data for predicting purchase behavior\n",
    "n_samples = 1000\n",
    "\n",
    "# Customer features\n",
    "age = np.random.normal(35, 12, n_samples)\n",
    "age = np.clip(age, 18, 70)  # Realistic age range\n",
    "income = np.random.normal(50000, 20000, n_samples)\n",
    "income = np.clip(income, 20000, 150000)  # Realistic income range\n",
    "time_on_site = np.random.exponential(5, n_samples)  # Minutes on website\n",
    "pages_visited = np.random.poisson(3, n_samples) + 1\n",
    "previous_purchases = np.random.poisson(2, n_samples)\n",
    "days_since_last_visit = np.random.exponential(10, n_samples)\n",
    "\n",
    "# Create realistic relationships for purchase decision\n",
    "# Higher income, more time on site, more pages visited = higher purchase probability\n",
    "purchase_probability = (\n",
    "    0.3 * (income / 50000) +\n",
    "    0.2 * (time_on_site / 10) +\n",
    "    0.2 * (pages_visited / 5) +\n",
    "    0.15 * (previous_purchases / 3) +\n",
    "    0.1 * (age / 50) +\n",
    "    0.05 * (1 / (days_since_last_visit + 1))\n",
    ")\n",
    "\n",
    "# Add some noise and create binary outcome\n",
    "purchase_probability += np.random.normal(0, 0.1, n_samples)\n",
    "purchase_probability = np.clip(purchase_probability, 0, 1)\n",
    "will_purchase = (purchase_probability > 0.5).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "customer_data = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Income': income,\n",
    "    'Time_on_Site': time_on_site,\n",
    "    'Pages_Visited': pages_visited,\n",
    "    'Previous_Purchases': previous_purchases,\n",
    "    'Days_Since_Last_Visit': days_since_last_visit,\n",
    "    'Will_Purchase': will_purchase\n",
    "})\n",
    "\n",
    "print(\"Customer Purchase Prediction Dataset:\")\n",
    "print(customer_data.head())\n",
    "print(f\"\\nDataset shape: {customer_data.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(customer_data['Will_Purchase'].value_counts())\n",
    "print(f\"\\nClass balance: {customer_data['Will_Purchase'].mean():.2%} positive class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis for Classification\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Feature distributions by class\n",
    "features = ['Age', 'Income', 'Time_on_Site', 'Pages_Visited', 'Previous_Purchases', 'Days_Since_Last_Visit']\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    # Create histogram for each class\n",
    "    axes[row, col].hist(customer_data[customer_data['Will_Purchase'] == 0][feature], \n",
    "                       alpha=0.7, label='No Purchase', bins=30, color='red')\n",
    "    axes[row, col].hist(customer_data[customer_data['Will_Purchase'] == 1][feature], \n",
    "                       alpha=0.7, label='Purchase', bins=30, color='green')\n",
    "    axes[row, col].set_title(f'Distribution of {feature} by Class')\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = customer_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "           square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "\n",
    "Logistic regression is a linear classifier that uses the logistic function to model the probability of class membership.\n",
    "\n",
    "### Mathematical Foundation:\n",
    "```\n",
    "P(y=1|x) = 1 / (1 + e^(-(β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ)))\n",
    "```\n",
    "\n",
    "### Key Properties:\n",
    "- Outputs probabilities between 0 and 1\n",
    "- Linear decision boundary\n",
    "- Assumes linear relationship between features and log-odds\n",
    "- Requires feature scaling for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"=== LOGISTIC REGRESSION ===\")\n",
    "\n",
    "# Separate features and target\n",
    "X = customer_data.drop('Will_Purchase', axis=1)\n",
    "y = customer_data['Will_Purchase']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba_log = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
    "precision_log = precision_score(y_test, y_pred_log)\n",
    "recall_log = recall_score(y_test, y_pred_log)\n",
    "f1_log = f1_score(y_test, y_pred_log)\n",
    "auc_log = roc_auc_score(y_test, y_pred_proba_log)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_log:.4f}\")\n",
    "print(f\"Precision: {precision_log:.4f}\")\n",
    "print(f\"Recall: {recall_log:.4f}\")\n",
    "print(f\"F1-Score: {f1_log:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_log:.4f}\")\n",
    "\n",
    "# Display feature importance (coefficients)\n",
    "print(\"\\nFeature Importance (Coefficients):\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': log_reg.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(log_reg.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Trees\n",
    "\n",
    "Decision trees make predictions by learning simple decision rules inferred from data features. They create a tree-like model of decisions.\n",
    "\n",
    "### Key Properties:\n",
    "- Easy to understand and interpret\n",
    "- Handles both numerical and categorical data\n",
    "- No need for feature scaling\n",
    "- Prone to overfitting\n",
    "- Non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Implementation\n",
    "print(\"=== DECISION TREE ===\")\n",
    "\n",
    "# Train decision tree (no scaling needed)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "y_pred_proba_dt = dt_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "precision_dt = precision_score(y_test, y_pred_dt)\n",
    "recall_dt = recall_score(y_test, y_pred_dt)\n",
    "f1_dt = f1_score(y_test, y_pred_dt)\n",
    "auc_dt = roc_auc_score(y_test, y_pred_proba_dt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_dt:.4f}\")\n",
    "print(f\"Precision: {precision_dt:.4f}\")\n",
    "print(f\"Recall: {recall_dt:.4f}\")\n",
    "print(f\"F1-Score: {f1_dt:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_dt:.4f}\")\n",
    "\n",
    "# Display feature importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_importance_dt = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': dt_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision tree\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(dt_clf, \n",
    "          feature_names=X.columns,\n",
    "          class_names=['No Purchase', 'Purchase'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Decision Tree Visualization')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_dt['Feature'], feature_importance_dt['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Decision Tree Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest\n",
    "\n",
    "Random Forest is an ensemble method that combines multiple decision trees to create a more robust classifier.\n",
    "\n",
    "### Key Properties:\n",
    "- Reduces overfitting compared to single decision trees\n",
    "- Handles missing values well\n",
    "- Provides feature importance\n",
    "- Can handle large datasets efficiently\n",
    "- Generally achieves high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Implementation\n",
    "print(\"=== RANDOM FOREST ===\")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "y_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Precision: {precision_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n",
    "print(f\"F1-Score: {f1_rf:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_rf:.4f}\")\n",
    "\n",
    "# Display feature importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. k-Nearest Neighbors (k-NN)\n",
    "\n",
    "k-NN is a lazy learning algorithm that classifies new instances based on the majority class among the k nearest neighbors.\n",
    "\n",
    "### Key Properties:\n",
    "- Simple and intuitive\n",
    "- No training phase (lazy learning)\n",
    "- Sensitive to feature scaling\n",
    "- Can be computationally expensive for large datasets\n",
    "- Works well with small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Implementation\n",
    "print(\"=== K-NEAREST NEIGHBORS ===\")\n",
    "\n",
    "# Train k-NN (using scaled features)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn_clf.predict(X_test_scaled)\n",
    "y_pred_proba_knn = knn_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "precision_knn = precision_score(y_test, y_pred_knn)\n",
    "recall_knn = recall_score(y_test, y_pred_knn)\n",
    "f1_knn = f1_score(y_test, y_pred_knn)\n",
    "auc_knn = roc_auc_score(y_test, y_pred_proba_knn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_knn:.4f}\")\n",
    "print(f\"Precision: {precision_knn:.4f}\")\n",
    "print(f\"Recall: {recall_knn:.4f}\")\n",
    "print(f\"F1-Score: {f1_knn:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_knn:.4f}\")\n",
    "\n",
    "# Test different k values\n",
    "print(\"\\n=== TESTING DIFFERENT K VALUES ===\")\n",
    "k_values = range(1, 21)\n",
    "k_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_temp.fit(X_train_scaled, y_train)\n",
    "    y_pred_temp = knn_temp.predict(X_test_scaled)\n",
    "    accuracy_temp = accuracy_score(y_test, y_pred_temp)\n",
    "    k_scores.append(accuracy_temp)\n",
    "\n",
    "# Find best k\n",
    "best_k = k_values[np.argmax(k_scores)]\n",
    "best_accuracy = max(k_scores)\n",
    "print(f\"Best k: {best_k} with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Plot k vs accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, k_scores, 'bo-')\n",
    "plt.axvline(best_k, color='red', linestyle='--', alpha=0.7, label=f'Best k={best_k}')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('k-NN Performance vs k Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification Metrics Deep Dive\n",
    "\n",
    "Understanding evaluation metrics is crucial for classification problems. Different metrics emphasize different aspects of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison\n",
    "print(\"=== COMPREHENSIVE MODEL COMPARISON ===\")\n",
    "\n",
    "# Store all results\n",
    "results = {\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'k-NN'],\n",
    "    'Accuracy': [accuracy_log, accuracy_dt, accuracy_rf, accuracy_knn],\n",
    "    'Precision': [precision_log, precision_dt, precision_rf, precision_knn],\n",
    "    'Recall': [recall_log, recall_dt, recall_rf, recall_knn],\n",
    "    'F1-Score': [f1_log, f1_dt, f1_rf, f1_knn],\n",
    "    'AUC-ROC': [auc_log, auc_dt, auc_rf, auc_knn]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Best model\n",
    "best_model_idx = results_df['F1-Score'].idxmax()\n",
    "best_model = results_df.iloc[best_model_idx]['Model']\n",
    "print(f\"\\nBest model based on F1-Score: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "models = {\n",
    "    'Logistic Regression': y_pred_log,\n",
    "    'Decision Tree': y_pred_dt,\n",
    "    'Random Forest': y_pred_rf,\n",
    "    'k-NN': y_pred_knn\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (model_name, predictions) in enumerate(models.items()):\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "    axes[i].set_title(f'{model_name} Confusion Matrix')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate ROC curves for all models\n",
    "probabilities = {\n",
    "    'Logistic Regression': y_pred_proba_log,\n",
    "    'Decision Tree': y_pred_proba_dt,\n",
    "    'Random Forest': y_pred_proba_rf,\n",
    "    'k-NN': y_pred_proba_knn\n",
    "}\n",
    "\n",
    "for model_name, proba in probabilities.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    auc_score = roc_auc_score(y_test, proba)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.7, label='Random Classifier')\n",
    "\n",
"plt.xlabel('False Positive Rate')\n",
"plt.ylabel('True Positive Rate')\n",
"plt.title('ROC Curves Comparison')\n",
"plt.legend()\n",
"plt.grid(True, alpha=0.3)\n",
"plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "Let's optimize our models using GridSearchCV to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"=== HYPERPARAMETER TUNING FOR RANDOM FOREST ===\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), \n",
    "                             param_grid_rf, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"Best parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best CV F1-Score: {grid_search_rf.best_score_:.4f}\")\n",
    "\n",
    "# Test best model\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "f1_best_rf = f1_score(y_test, y_pred_best_rf)\n",
    "print(f\"Test F1-Score: {f1_best_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Congratulations! You've mastered the fundamentals of classification algorithms. Here's what you learned:\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "1. **Classification vs Regression**: Understanding the difference between predicting categories vs continuous values\n",
    "2. **Logistic Regression**: Linear classifier using the logistic function\n",
    "3. **Decision Trees**: Tree-based models that create interpretable decision rules\n",
    "4. **Random Forest**: Ensemble method combining multiple decision trees\n",
    "5. **k-NN**: Instance-based learning using nearest neighbors\n",
    "6. **Classification Metrics**: Accuracy, precision, recall, F1-score, and AUC-ROC\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- Implementing various classification algorithms\n",
    "- Understanding when to use each algorithm\n",
    "- Evaluating classification performance with appropriate metrics\n",
    "- Interpreting confusion matrices and ROC curves\n",
    "- Tuning hyperparameters for optimal performance\n",
    "- Handling imbalanced datasets\n",
    "\n",
    "### When to Use Each Algorithm:\n",
    "- **Logistic Regression**: When you need interpretable linear relationships and probability estimates\n",
    "- **Decision Trees**: When you need highly interpretable models and can handle overfitting\n",
    "- **Random Forest**: When you want high accuracy with built-in feature importance\n",
    "- **k-NN**: When you have small datasets and local patterns are important\n",
    "\n",
    "### Classification Metrics Guide:\n",
    "- **Accuracy**: Overall correctness (good for balanced datasets)\n",
    "- **Precision**: How many selected items are relevant (minimize false positives)\n",
    "- **Recall**: How many relevant items are selected (minimize false negatives)\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced metric)\n",
    "- **AUC-ROC**: Overall discrimination ability across all thresholds\n",
    "\n",
    "### Best Practices:\n",
    "- Always check class balance and consider stratified sampling\n",
    "- Use appropriate metrics based on your problem (precision vs recall trade-off)\n",
    "- Scale features for algorithms that need it (logistic regression, k-NN)\n",
    "- Use cross-validation for reliable performance estimates\n",
    "- Consider ensemble methods for better performance\n",
    "- Visualize decision boundaries when possible\n",
    "\n",
    "### Next Steps:\n",
    "In the next week, we'll explore unsupervised learning including clustering algorithms and dimensionality reduction techniques like PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
