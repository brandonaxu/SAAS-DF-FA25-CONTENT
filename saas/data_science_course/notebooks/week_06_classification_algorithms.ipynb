{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Unsupervised Learning\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand clustering algorithms\n",
    "- Learn dimensionality reduction techniques\n",
    "- Apply unsupervised learning to real data\n",
    "- Understand when to use unsupervised methods\n",
    "\n",
    "## Topics Covered:\n",
    "- K-means clustering\n",
    "- Principal Component Analysis (PCA)\n",
    "- Hierarchical clustering\n",
    "- DBSCAN clustering\n",
    "- Cluster evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Unsupervised Learning\n",
    "\n",
    "Unsupervised learning finds hidden patterns in data without labeled examples. Unlike supervised learning, we don't have target variables to predict.\n",
    "\n",
    "### Main Types:\n",
    "1. **Clustering**: Grouping similar data points together\n",
    "2. **Dimensionality Reduction**: Reducing the number of features while preserving information\n",
    "3. **Association Rules**: Finding relationships between variables\n",
    "4. **Anomaly Detection**: Identifying unusual data points\n",
    "\n",
    "### Applications:\n",
    "- Customer segmentation\n",
    "- Market basket analysis\n",
    "- Data compression\n",
    "- Feature engineering\n",
    "- Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic customer segmentation dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Generate customer features\n",
    "age = np.random.normal(40, 15, n_samples)\n",
    "age = np.clip(age, 18, 80)\n",
    "income = np.random.normal(50000, 25000, n_samples)\n",
    "income = np.clip(income, 20000, 200000)\n",
    "spending_score = np.random.normal(50, 20, n_samples)\n",
    "spending_score = np.clip(spending_score, 1, 100)\n",
    "annual_spending = np.random.normal(2000, 1000, n_samples)\n",
    "annual_spending = np.clip(annual_spending, 200, 10000)\n",
    "\n",
    "# Create some realistic relationships\n",
    "# Higher income generally leads to higher spending\n",
    "annual_spending = annual_spending + (income / 50000) * 1000 + np.random.normal(0, 500, n_samples)\n",
    "# Younger people might have different spending patterns\n",
    "spending_score = spending_score + (80 - age) * 0.3 + np.random.normal(0, 5, n_samples)\n",
    "spending_score = np.clip(spending_score, 1, 100)\n",
    "\n",
    "# Create DataFrame\n",
    "customer_data = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Income': income,\n",
    "    'Spending_Score': spending_score,\n",
    "    'Annual_Spending': annual_spending\n",
    "})\n",
    "\n",
    "print(\"Customer Segmentation Dataset:\")\n",
    "print(customer_data.head())\n",
    "print(f\"\\nDataset shape: {customer_data.shape}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(customer_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Distribution plots\n",
    "customer_data['Age'].hist(bins=30, ax=axes[0, 0], alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "customer_data['Income'].hist(bins=30, ax=axes[0, 1], alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Income Distribution')\n",
    "axes[0, 1].set_xlabel('Income')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "customer_data['Spending_Score'].hist(bins=30, ax=axes[1, 0], alpha=0.7, color='salmon')\n",
    "axes[1, 0].set_title('Spending Score Distribution')\n",
    "axes[1, 0].set_xlabel('Spending Score')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "customer_data['Annual_Spending'].hist(bins=30, ax=axes[1, 1], alpha=0.7, color='gold')\n",
    "axes[1, 1].set_title('Annual Spending Distribution')\n",
    "axes[1, 1].set_xlabel('Annual Spending')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = customer_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "           square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-means Clustering\n",
    "\n",
    "K-means is one of the most popular clustering algorithms. It partitions data into k clusters by minimizing the within-cluster sum of squares.\n",
    "\n",
    "### How it works:\n",
    "1. Choose number of clusters (k)\n",
    "2. Initialize k cluster centroids randomly\n",
    "3. Assign each point to nearest centroid\n",
    "4. Update centroids to mean of assigned points\n",
    "5. Repeat steps 3-4 until convergence\n",
    "\n",
    "### Key Properties:\n",
    "- Requires specifying k in advance\n",
    "- Assumes spherical clusters\n",
    "- Sensitive to initialization\n",
    "- Scales well to large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "print(\"=== K-MEANS CLUSTERING ===\")\n",
    "\n",
    "# Scale the features (important for K-means)\n",
    "scaler = StandardScaler()\n",
    "customer_scaled = scaler.fit_transform(customer_data)\n",
    "\n",
    "# Find optimal number of clusters using elbow method\n",
    "k_range = range(1, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(customer_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    if k > 1:  # Silhouette score requires at least 2 clusters\n",
    "        silhouette_avg = silhouette_score(customer_scaled, kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "    else:\n",
    "        silhouette_scores.append(0)\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Elbow method\n",
    "axes[0].plot(k_range, inertias, 'bo-')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia (WCSS)')\n",
    "axes[0].set_title('Elbow Method for Optimal k')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette analysis\n",
    "axes[1].plot(k_range, silhouette_scores, 'ro-')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis for Optimal k')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best k based on silhouette score\n",
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal k based on silhouette score: {best_k}\")\n",
    "print(f\"Best silhouette score: {max(silhouette_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means with optimal k\n",
    "optimal_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "cluster_labels = optimal_kmeans.fit_predict(customer_scaled)\n",
    "\n",
    "# Add cluster labels to original data\n",
    "customer_data['Cluster'] = cluster_labels\n",
    "\n",
    "# Analyze clusters\n",
    "print(f\"\\n=== CLUSTER ANALYSIS (K={best_k}) ===\")\n",
    "print(\"Cluster sizes:\")\n",
    "print(customer_data['Cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCluster centroids (original scale):\")\n",
    "cluster_centers = pd.DataFrame(\n",
    "    scaler.inverse_transform(optimal_kmeans.cluster_centers_),\n",
    "    columns=customer_data.columns[:-1]\n",
    ")\n",
    "cluster_centers['Cluster'] = range(best_k)\n",
    "print(cluster_centers.round(2))\n",
    "\n",
    "print(\"\\nCluster statistics:\")\n",
    "cluster_stats = customer_data.groupby('Cluster').agg({\n",
    "    'Age': ['mean', 'std'],\n",
    "    'Income': ['mean', 'std'],\n",
    "    'Spending_Score': ['mean', 'std'],\n",
    "    'Annual_Spending': ['mean', 'std']\n",
    "})\n",
    "print(cluster_stats.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Different feature combinations\n",
    "feature_pairs = [\n",
    "    ('Age', 'Income'),\n",
    "    ('Age', 'Spending_Score'),\n",
    "    ('Income', 'Annual_Spending'),\n",
    "    ('Spending_Score', 'Annual_Spending')\n",
    "]\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "\n",
    "for i, (x_feat, y_feat) in enumerate(feature_pairs):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for cluster in range(best_k):\n",
    "        cluster_data = customer_data[customer_data['Cluster'] == cluster]\n",
    "        axes[row, col].scatter(cluster_data[x_feat], cluster_data[y_feat], \n",
    "                              c=colors[cluster], alpha=0.6, s=50, \n",
    "                              label=f'Cluster {cluster}')\n",
    "    \n",
    "    # Plot centroids\n",
    "    for cluster in range(best_k):\n",
    "        centroid = cluster_centers.iloc[cluster]\n",
    "        axes[row, col].scatter(centroid[x_feat], centroid[y_feat], \n",
    "                              c='black', marker='x', s=200, linewidths=3)\n",
    "    \n",
    "    axes[row, col].set_xlabel(x_feat)\n",
    "    axes[row, col].set_ylabel(y_feat)\n",
    "    axes[row, col].set_title(f'Clusters: {x_feat} vs {y_feat}')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that transforms data to a lower-dimensional space while preserving as much variance as possible.\n",
    "\n",
    "### How it works:\n",
    "1. Standardize the data\n",
    "2. Calculate covariance matrix\n",
    "3. Find eigenvectors and eigenvalues\n",
    "4. Sort by eigenvalues (descending)\n",
    "5. Transform data using selected components\n",
    "\n",
    "### Key Properties:\n",
    "- Linear transformation\n",
    "- Components are orthogonal\n",
    "- Preserves maximum variance\n",
    "- Useful for visualization and feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "print(\"=== PRINCIPAL COMPONENT ANALYSIS ===\")\n",
    "\n",
    "# Apply PCA to the customer data\n",
    "pca = PCA()\n",
    "customer_pca = pca.fit_transform(customer_scaled)\n",
    "\n",
    "# Analyze explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(\"Explained variance ratio for each component:\")\n",
    "for i, ratio in enumerate(explained_variance_ratio):\n",
    "    print(f\"PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative explained variance:\")\n",
    "for i, cum_var in enumerate(cumulative_variance):\n",
    "    print(f\"PC1-PC{i+1}: {cum_var:.4f} ({cum_var*100:.2f}%)\")\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Individual explained variance\n",
    "axes[0].bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Explained Variance by Component')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "axes[1].axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='80% variance')\n",
    "axes[1].axhline(y=0.95, color='g', linestyle='--', alpha=0.7, label='95% variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA components\n",
    "print(\"\\n=== PCA COMPONENT ANALYSIS ===\")\n",
    "\n",
    "# Component loadings (weights)\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(len(pca.components_))],\n",
    "    index=customer_data.columns[:-1]\n",
    ")\n",
    "\n",
    "print(\"Component loadings:\")\n",
    "print(components_df.round(3))\n",
    "\n",
    "# Visualize first two components\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Component loadings heatmap\n",
    "sns.heatmap(components_df.iloc[:, :2], annot=True, cmap='RdBu_r', center=0,\n",
    "           ax=axes[0], cbar_kws={'shrink': 0.8})\n",
    "axes[0].set_title('Component Loadings (PC1 & PC2)')\n",
    "\n",
    "# 2D PCA visualization with clusters\n",
    "for cluster in range(best_k):\n",
    "    cluster_mask = cluster_labels == cluster\n",
    "    axes[1].scatter(customer_pca[cluster_mask, 0], customer_pca[cluster_mask, 1], \n",
    "                   c=colors[cluster], alpha=0.6, s=50, label=f'Cluster {cluster}')\n",
    "\n",
    "axes[1].set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}% variance)')\n",
    "axes[1].set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}% variance)')\n",
    "axes[1].set_title('Data in PCA Space (Colored by K-means Clusters)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering creates a tree-like structure of clusters. It can be agglomerative (bottom-up) or divisive (top-down).\n",
    "\n",
    "### Agglomerative Clustering:\n",
    "1. Start with each point as its own cluster\n",
    "2. Merge closest clusters\n",
    "3. Repeat until one cluster remains\n",
    "\n",
    "### Key Properties:\n",
    "- No need to specify number of clusters in advance\n",
    "- Creates dendrogram showing cluster hierarchy\n",
    "- Different linkage methods (single, complete, average, ward)\n",
    "- Can handle non-spherical clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering\n",
    "print(\"=== HIERARCHICAL CLUSTERING ===\")\n",
    "\n",
    "# For visualization, use a subset of data\n",
    "n_sample = 100\n",
    "sample_indices = np.random.choice(len(customer_scaled), n_sample, replace=False)\n",
    "customer_sample = customer_scaled[sample_indices]\n",
    "\n",
    "# Create linkage matrix\n",
    "linkage_matrix = linkage(customer_sample, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=10)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index or (Cluster Size)')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Apply hierarchical clustering to full dataset\n",
    "n_clusters = best_k\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "hierarchical_labels = hierarchical.fit_predict(customer_scaled)\n",
    "\n",
    "# Compare with K-means\n",
    "print(f\"\\nCluster comparison (K-means vs Hierarchical):\")\n",
    "print(f\"K-means cluster sizes: {np.bincount(cluster_labels)}\")\n",
    "print(f\"Hierarchical cluster sizes: {np.bincount(hierarchical_labels)}\")\n",
    "\n",
    "# Calculate similarity between clusterings\n",
    "ari_score = adjusted_rand_score(cluster_labels, hierarchical_labels)\n",
    "print(f\"\\nAdjusted Rand Index (similarity): {ari_score:.4f}\")\n",
    "\n",
    "# Silhouette score for hierarchical clustering\n",
    "hierarchical_silhouette = silhouette_score(customer_scaled, hierarchical_labels)\n",
    "print(f\"Hierarchical clustering silhouette score: {hierarchical_silhouette:.4f}\")\n",
    "print(f\"K-means silhouette score: {silhouette_score(customer_scaled, cluster_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DBSCAN Clustering\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups together points that are closely packed and marks outliers.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Core points**: Points with at least min_samples neighbors within eps distance\n",
    "- **Border points**: Points within eps distance of core points\n",
    "- **Noise points**: Points that are neither core nor border\n",
    "\n",
    "### Key Properties:\n",
    "- Automatically determines number of clusters\n",
    "- Can find arbitrarily shaped clusters\n",
    "- Identifies outliers as noise\n",
    "- Sensitive to hyperparameters (eps, min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Clustering\n",
    "print(\"=== DBSCAN CLUSTERING ===\")\n",
    "\n",
    "# Try different eps values\n",
    "eps_values = [0.3, 0.5, 0.7, 1.0, 1.5]\n",
    "min_samples = 5\n",
    "\n",
    "dbscan_results = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan_labels = dbscan.fit_predict(customer_scaled)\n",
    "    \n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    n_noise = list(dbscan_labels).count(-1)\n",
    "    \n",
    "    if n_clusters > 1:\n",
    "        silhouette = silhouette_score(customer_scaled, dbscan_labels)\n",
    "    else:\n",
    "        silhouette = -1\n",
    "    \n",
    "    dbscan_results.append({\n",
    "        'eps': eps,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'silhouette': silhouette,\n",
    "        'labels': dbscan_labels\n",
    "    })\n",
    "    \n",
    "    print(f\"eps={eps}: {n_clusters} clusters, {n_noise} noise points, silhouette={silhouette:.4f}\")\n",
    "\n",
    "# Choose best eps based on silhouette score\n",
    "best_dbscan = max(dbscan_results, key=lambda x: x['silhouette'])\n",
    "print(f\"\\nBest DBSCAN: eps={best_dbscan['eps']}, silhouette={best_dbscan['silhouette']:.4f}\")\n",
    "\n",
    "# Visualize DBSCAN results\n",
    "dbscan_labels = best_dbscan['labels']\n",
    "unique_labels = set(dbscan_labels)\n",
    "n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
"\n",
"# Plot DBSCAN results\n",
"core_samples_mask = np.zeros_like(dbscan_labels, dtype=bool)\n",
"for i, label in enumerate(dbscan_labels):\n",
"    if label != -1:\n",
"        core_samples_mask[i] = True\n",
"\n",
"# Use first two PCA components for visualization\n",
"pca_2d = PCA(n_components=2)\n",
"customer_pca_2d = pca_2d.fit_transform(customer_scaled)\n",
"\n",
"# Plot clusters\n",
"unique_labels = set(dbscan_labels)\n",
"colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
"\n",
"for k, col in zip(unique_labels, colors):\n",
"    if k == -1:\n",
"        # Noise points\n",
"        col = 'black'\n",
"        marker = 'x'\n",
"        alpha = 0.3\n",
"        label = 'Noise'\n",
"    else:\n",
"        marker = 'o'\n",
"        alpha = 0.6\n",
"        label = f'Cluster {k}'\n",
"    \n",
"    class_member_mask = (dbscan_labels == k)\n",
"    xy = customer_pca_2d[class_member_mask]\n",
"    plt.scatter(xy[:, 0], xy[:, 1], c=col, marker=marker, alpha=alpha, s=50, label=label)\n",
"\n",
"plt.title(f'DBSCAN Clustering (eps={best_dbscan[\"eps\"]}, min_samples={min_samples})')\n",
"plt.xlabel('PC1')\n",
"plt.ylabel('PC2')\n",
"plt.legend()\n",
"plt.grid(True, alpha=0.3)\n",
"plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering Comparison\n",
    "\n",
    "Let's compare all the clustering methods we've implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all clustering methods\n",
    "print(\"=== CLUSTERING METHODS COMPARISON ===\")\n",
    "\n",
    "# Prepare results\n",
    "clustering_results = {\n",
    "    'K-means': {\n",
    "        'labels': cluster_labels,\n",
    "        'silhouette': silhouette_score(customer_scaled, cluster_labels),\n",
    "        'n_clusters': best_k,\n",
    "        'n_noise': 0\n",
    "    },\n",
    "    'Hierarchical': {\n",
    "        'labels': hierarchical_labels,\n",
    "        'silhouette': hierarchical_silhouette,\n",
    "        'n_clusters': best_k,\n",
    "        'n_noise': 0\n",
    "    },\n",
    "    'DBSCAN': {\n",
    "        'labels': dbscan_labels,\n",
    "        'silhouette': best_dbscan['silhouette'],\n",
    "        'n_clusters': best_dbscan['n_clusters'],\n",
    "        'n_noise': best_dbscan['n_noise']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print comparison\n",
    "print(f\"{'Method':<12} {'Clusters':<10} {'Noise':<8} {'Silhouette':<12}\")\n",
    "print(\"-\" * 45)\n",
    "for method, results in clustering_results.items():\n",
    "    print(f\"{method:<12} {results['n_clusters']:<10} {results['n_noise']:<8} {results['silhouette']:<12.4f}\")\n",
    "\n",
    "# Find best method\n",
    "best_method = max(clustering_results.items(), key=lambda x: x[1]['silhouette'])\n",
    "print(f\"\\nBest method based on silhouette score: {best_method[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Customer Segmentation Insights\n",
    "\n",
    "Let's interpret our clustering results for business insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business insights from clustering\n",
    "print(\"=== CUSTOMER SEGMENTATION INSIGHTS ===\")\n",
    "\n",
    "# Use K-means results for interpretation\n",
    "cluster_profiles = customer_data.groupby('Cluster').agg({\n",
    "    'Age': 'mean',\n",
    "    'Income': 'mean',\n",
    "    'Spending_Score': 'mean',\n",
    "    'Annual_Spending': 'mean'\n",
    "}).round(0)\n",
    "\n",
    "print(\"\\nCluster Profiles:\")\n",
    "print(cluster_profiles)\n",
    "\n",
    "# Create business-friendly cluster names\n",
    "cluster_names = {}\n",
    "for cluster in range(best_k):\n",
    "    profile = cluster_profiles.iloc[cluster]\n",
    "    \n",
    "    # Simple heuristic for naming\n",
    "    if profile['Income'] > 60000 and profile['Spending_Score'] > 60:\n",
    "        name = 'High-Value Customers'\n",
    "    elif profile['Income'] > 60000 and profile['Spending_Score'] <= 60:\n",
    "        name = 'Conservative Spenders'\n",
    "    elif profile['Income'] <= 60000 and profile['Spending_Score'] > 60:\n",
    "        name = 'Enthusiastic Spenders'\n",
    "    elif profile['Age'] > 50:\n",
    "        name = 'Mature Customers'\n",
    "    else:\n",
    "        name = 'Budget-Conscious'\n",
    "    \n",
    "    cluster_names[cluster] = name\n",
    "\n",
    "print(\"\\nBusiness Segment Names:\")\n",
    "for cluster, name in cluster_names.items():\n",
    "    size = (customer_data['Cluster'] == cluster).sum()\n",
    "    percentage = (size / len(customer_data)) * 100\n",
    "    print(f\"Cluster {cluster}: {name} ({size} customers, {percentage:.1f}%)\")\n",
    "\n",
    "# Marketing recommendations\n",
    "print(\"\\n=== MARKETING RECOMMENDATIONS ===\")\n",
    "for cluster, name in cluster_names.items():\n",
    "    profile = cluster_profiles.iloc[cluster]\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  - Average Age: {profile['Age']:.0f}\")\n",
    "    print(f\"  - Average Income: ${profile['Income']:,.0f}\")\n",
    "    print(f\"  - Spending Score: {profile['Spending_Score']:.0f}/100\")\n",
    "    print(f\"  - Annual Spending: ${profile['Annual_Spending']:,.0f}\")\n",
    "    \n",
    "    # Simple recommendations\n",
    "    if 'High-Value' in name:\n",
    "        print(f\"  - Strategy: Premium products, VIP treatment, loyalty programs\")\n",
    "    elif 'Conservative' in name:\n",
    "        print(f\"  - Strategy: Value propositions, quality assurance, long-term benefits\")\n",
    "    elif 'Enthusiastic' in name:\n",
    "        print(f\"  - Strategy: Trendy products, social media engagement, exclusive access\")\n",
    "    elif 'Mature' in name:\n",
    "        print(f\"  - Strategy: Traditional channels, customer service, reliability\")\n",
    "    else:\n",
    "        print(f\"  - Strategy: Discounts, value products, price promotions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Congratulations! You've mastered the fundamentals of unsupervised learning. Here's what you learned:\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "1. **Clustering**: Grouping similar data points without labels\n",
    "2. **K-means**: Partitioning data into k spherical clusters\n",
    "3. **Hierarchical Clustering**: Creating tree-like cluster structures\n",
    "4. **DBSCAN**: Density-based clustering that finds arbitrary shapes and outliers\n",
    "5. **PCA**: Dimensionality reduction while preserving variance\n",
    "6. **Cluster Evaluation**: Using silhouette score and other metrics\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- Implementing various clustering algorithms\n",
    "- Determining optimal number of clusters\n",
    "- Evaluating clustering performance\n",
    "- Applying dimensionality reduction techniques\n",
    "- Interpreting clusters for business insights\n",
    "- Choosing appropriate algorithms for different data types\n",
    "\n",
    "### When to Use Each Algorithm:\n",
    "- **K-means**: When you know the number of clusters and expect spherical clusters\n",
    "- **Hierarchical**: When you want to explore different numbers of clusters\n",
    "- **DBSCAN**: When you have irregular cluster shapes and want to identify outliers\n",
    "- **PCA**: When you need dimensionality reduction for visualization or feature engineering\n",
    "\n",
    "### Best Practices:\n",
    "- Always scale your features before clustering\n",
    "- Use multiple methods to validate results\n",
    "- Consider domain knowledge when interpreting clusters\n",
    "- Evaluate clustering quality with appropriate metrics\n",
    "- Visualize results to understand cluster structure\n",
    "- Test different hyperparameters systematically\n",
    "\n",
    "### Real-world Applications:\n",
    "- Customer segmentation for targeted marketing\n",
    "- Market research and consumer behavior analysis\n",
    "- Image segmentation and computer vision\n",
    "- Gene expression analysis in bioinformatics\n",
    "- Social network analysis\n",
    "- Anomaly detection in cybersecurity\n",
    "\n",
    "### Next Steps:\n",
    "In the next week, we'll explore general machine learning techniques including ensemble methods, hyperparameter tuning, and model interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
