{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: General Machine Learning Techniques\n",
    "\n",
    "## Learning Objectives:\n",
    "- Learn ensemble methods\n",
    "- Understand model selection and tuning\n",
    "- Explore advanced evaluation techniques\n",
    "- Handle imbalanced datasets\n",
    "\n",
    "## Topics Covered:\n",
    "- Ensemble methods (bagging, boosting)\n",
    "- Gradient boosting (XGBoost, LightGBM)\n",
    "- Hyperparameter tuning\n",
    "- Grid search and random search\n",
    "- Handling imbalanced data\n",
    "- Feature importance and selection\n",
    "- Model interpretability (SHAP, LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced libraries (may need installation)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"XGBoost not available - install with: pip install xgboost\")\n",
    "    xgb = None\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"LightGBM available\")\n",
    "except ImportError:\n",
    "    print(\"LightGBM not available - install with: pip install lightgbm\")\n",
    "    lgb = None\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    print(\"SHAP available\")\n",
    "except ImportError:\n",
    "    print(\"SHAP not available - install with: pip install shap\")\n",
    "    shap = None\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Ensemble Methods\n",
    "\n",
    "Ensemble methods combine multiple models to create stronger predictors. The key insight is that combining weak learners can create strong learners.\n",
    "\n",
    "### Types of Ensemble Methods:\n",
    "1. **Bagging**: Bootstrap Aggregating - trains models on different subsets of data\n",
    "2. **Boosting**: Sequential training where each model corrects previous errors\n",
    "3. **Stacking**: Uses a meta-model to combine predictions from base models\n",
    "4. **Voting**: Combines predictions through majority vote or averaging\n",
    "\n",
    "### Benefits:\n",
    "- Reduces overfitting\n",
    "- Improves generalization\n",
    "- More robust predictions\n",
    "- Often achieves better performance than individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dataset for classification\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Dataset created:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Class balance: {y.mean():.2%} positive class\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bagging Methods\n",
    "\n",
    "Bagging (Bootstrap Aggregating) trains multiple models on different bootstrap samples of the training data.\n",
    "\n",
    "### Random Forest:\n",
    "- Bagging + Random feature selection\n",
    "- Each tree uses a random subset of features\n",
    "- Reduces correlation between trees\n",
    "- Provides feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging with Random Forest\n",
    "print(\"=== BAGGING: RANDOM FOREST ===\")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Compare with single Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n",
    "dt_auc = roc_auc_score(y_test, dt_pred_proba)\n",
    "\n",
    "print(f\"\\nSingle Decision Tree AUC: {dt_auc:.4f}\")\n",
    "print(f\"Improvement from Random Forest: {rf_auc - dt_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Bagging implementation\n",
    "print(\"\\n=== MANUAL BAGGING IMPLEMENTATION ===\")\n",
    "\n",
    "# Create bagging classifier\n",
    "bagging_model = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_pred_proba = bagging_model.predict_proba(X_test)[:, 1]\n",
    "bagging_auc = roc_auc_score(y_test, bagging_pred_proba)\n",
    "\n",
    "print(f\"Bagging AUC: {bagging_auc:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(10), feature_importance.head(10)['importance'])\n",
    "plt.yticks(range(10), feature_importance.head(10)['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Feature Importances (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting Methods\n",
    "\n",
    "Boosting trains models sequentially, where each model attempts to correct the errors of the previous ones.\n",
    "\n",
    "### Gradient Boosting:\n",
    "- Builds models sequentially\n",
    "- Each model predicts the residuals of the previous model\n",
    "- Combines weak learners into strong learners\n",
    "- Can overfit if not properly regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "print(\"=== BOOSTING: GRADIENT BOOSTING ===\")\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "gb_pred_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "gb_auc = roc_auc_score(y_test, gb_pred_proba)\n",
    "\n",
    "print(f\"Gradient Boosting AUC: {gb_auc:.4f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Training and validation scores\n",
    "train_scores = gb_model.train_score_\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_scores, label='Training Score')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Gradient Boosting Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance\n",
    "plt.subplot(1, 2, 2)\n",
    "gb_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.barh(range(10), gb_importance.head(10)['importance'])\n",
    "plt.yticks(range(10), gb_importance.head(10)['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Features (Gradient Boosting)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 most important features (Gradient Boosting):\")\n",
    "print(gb_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Boosting: XGBoost and LightGBM\n",
    "\n",
    "XGBoost and LightGBM are optimized implementations of gradient boosting with additional features:\n",
    "\n",
    "### XGBoost:\n",
    "- Regularization to prevent overfitting\n",
    "- Parallel processing\n",
    "- Missing value handling\n",
    "- Cross-validation support\n",
    "\n",
    "### LightGBM:\n",
    "- Faster training\n",
    "- Lower memory usage\n",
    "- Better accuracy\n",
    "- Handles categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost implementation\n",
    "if xgb is not None:\n",
    "    print(\"=== XGBOOST ===\")\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "    \n",
    "    print(f\"XGBoost AUC: {xgb_auc:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features (XGBoost):\")\n",
    "    print(xgb_importance.head(10))\n",
    "else:\n",
    "    print(\"XGBoost not available - skipping\")\n",
    "    xgb_auc = None\n",
    "\n",
    "# LightGBM implementation\n",
    "if lgb is not None:\n",
    "    print(\"\\n=== LIGHTGBM ===\")\n",
    "    \n",
    "    # Train LightGBM\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    lgb_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "    lgb_auc = roc_auc_score(y_test, lgb_pred_proba)\n",
    "    \n",
    "    print(f\"LightGBM AUC: {lgb_auc:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    lgb_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features (LightGBM):\")\n",
    "    print(lgb_importance.head(10))\n",
    "else:\n",
    "    print(\"LightGBM not available - skipping\")\n",
    "    lgb_auc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Voting Classifiers\n",
    "\n",
    "Voting classifiers combine different types of algorithms and use majority voting or averaging to make predictions.\n",
    "\n",
    "### Types:\n",
    "- **Hard Voting**: Uses predicted class labels\n",
    "- **Soft Voting**: Uses predicted probabilities (usually better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "print(\"=== VOTING CLASSIFIER ===\")\n",
    "\n",
    "# Create individual classifiers\n",
    "classifiers = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('lr', LogisticRegression(random_state=42, max_iter=1000))\n",
    "]\n",
    "\n",
    "# Add XGBoost if available\n",
    "if xgb is not None:\n",
    "    classifiers.append(('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')))\n",
    "\n",
    "# Create voting classifier\n",
    "voting_model = VotingClassifier(estimators=classifiers, voting='soft')\n",
    "voting_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "voting_pred_proba = voting_model.predict_proba(X_test_scaled)[:, 1]\n",
    "voting_auc = roc_auc_score(y_test, voting_pred_proba)\n",
    "\n",
    "print(f\"Voting Classifier AUC: {voting_auc:.4f}\")\n",
    "\n",
    "# Compare individual classifiers\n",
    "print(\"\\nIndividual classifier performance:\")\n",
    "for name, clf in classifiers:\n",
    "    if name == 'lr':\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    auc = roc_auc_score(y_test, pred_proba)\n",
    "    print(f\"{name.upper()}: {auc:.4f}\")\n",
    "\n",
    "print(f\"\\nVoting Classifier combines all: {voting_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is crucial for optimal model performance. We'll explore both grid search and random search.\n",
    "\n",
    "### Grid Search vs Random Search:\n",
    "- **Grid Search**: Exhaustive search over parameter grid\n",
    "- **Random Search**: Random sampling from parameter distributions\n",
    "- **Random Search**: Often more efficient for high-dimensional spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Random Forest\n",
    "print(\"=== GRID SEARCH HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf_pred_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "best_rf_auc = roc_auc_score(y_test, best_rf_pred_proba)\n",
    "\n",
    "print(f\"Test AUC with best parameters: {best_rf_auc:.4f}\")\n",
    "print(f\"Improvement over default: {best_rf_auc - rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search for comparison\n",
    "print(\"\\n=== RANDOM SEARCH HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "# Define parameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Random search\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_dist,\n",
    "    n_iter=50,  # Number of parameter combinations to try\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Test best model\n",
    "best_rf_random = random_search.best_estimator_\n",
    "best_rf_random_pred_proba = best_rf_random.predict_proba(X_test)[:, 1]\n",
    "best_rf_random_auc = roc_auc_score(y_test, best_rf_random_pred_proba)\n",
    "\n",
    "print(f\"Test AUC with random search: {best_rf_random_auc:.4f}\")\n",
    "\n",
    "# Compare search methods\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Grid Search AUC: {best_rf_auc:.4f}\")\n",
    "print(f\"Random Search AUC: {best_rf_random_auc:.4f}\")\n",
    "print(f\"Default RF AUC: {rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Imbalanced Data\n",
    "\n",
    "Real-world datasets often have imbalanced classes. We'll explore techniques to handle this:\n",
    "\n",
    "### Techniques:\n",
    "1. **Resampling**: Over-sampling minority class or under-sampling majority class\n",
    "2. **SMOTE**: Synthetic Minority Over-sampling Technique\n",
    "3. **Class Weights**: Penalize misclassification of minority class more\n",
    "4. **Cost-sensitive Learning**: Adjust decision threshold\n",
    "5. **Ensemble Methods**: Combine balanced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced dataset\n",
    "print(\"=== HANDLING IMBALANCED DATA ===\")\n",
    "\n",
    "# Create imbalanced version of our data\n",
    "# Keep all positive examples, randomly sample negative examples\n",
    "positive_indices = np.where(y_train == 1)[0]\n",
    "negative_indices = np.where(y_train == 0)[0]\n",
    "\n",
    "# Keep all positive examples and 20% of negative examples\n",
    "np.random.seed(42)\n",
    "selected_negative = np.random.choice(negative_indices, int(len(negative_indices) * 0.2), replace=False)\n",
    "imbalanced_indices = np.concatenate([positive_indices, selected_negative])\n",
    "\n",
    "X_train_imb = X_train[imbalanced_indices]\n",
    "y_train_imb = y_train[imbalanced_indices]\n",
    "\n",
    "print(f\"Original training set: {np.bincount(y_train)}\")\n",
    "print(f\"Imbalanced training set: {np.bincount(y_train_imb)}\")\n",
    "print(f\"Imbalance ratio: {y_train_imb.mean():.2%}\")\n",
"\n",
"# Train baseline model on imbalanced data\n",
"baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
"baseline_model.fit(X_train_imb, y_train_imb)\n",
"baseline_pred_proba = baseline_model.predict_proba(X_test)[:, 1]\n",
"baseline_auc = roc_auc_score(y_test, baseline_pred_proba)\n",
"\n",
"print(f\"\\nBaseline model (imbalanced data) AUC: {baseline_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "print(\"\\n=== METHOD 1: SMOTE ===\")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_imb, y_train_imb)\n",
    "\n",
    "print(f\"After SMOTE: {np.bincount(y_train_smote)}\")\n",
    "\n",
    "# Train model with SMOTE data\n",
    "smote_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "smote_model.fit(X_train_smote, y_train_smote)\n",
    "smote_pred_proba = smote_model.predict_proba(X_test)[:, 1]\n",
    "smote_auc = roc_auc_score(y_test, smote_pred_proba)\n",
    "\n",
    "print(f\"SMOTE model AUC: {smote_auc:.4f}\")\n",
    "print(f\"Improvement: {smote_auc - baseline_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Class Weights\n",
    "print(\"\\n=== METHOD 2: CLASS WEIGHTS ===\")\n",
    "\n",
    "# Train model with balanced class weights\n",
    "weighted_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "weighted_model.fit(X_train_imb, y_train_imb)\n",
    "weighted_pred_proba = weighted_model.predict_proba(X_test)[:, 1]\n",
    "weighted_auc = roc_auc_score(y_test, weighted_pred_proba)\n",
    "\n",
    "print(f\"Weighted model AUC: {weighted_auc:.4f}\")\n",
    "print(f\"Improvement: {weighted_auc - baseline_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Under-sampling\n",
    "print(\"\\n=== METHOD 3: UNDER-SAMPLING ===\")\n",
    "\n",
    "# Apply random under-sampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train_imb, y_train_imb)\n",
    "\n",
    "print(f\"After under-sampling: {np.bincount(y_train_under)}\")\n",
    "\n",
    "# Train model with under-sampled data\n",
    "under_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "under_model.fit(X_train_under, y_train_under)\n",
    "under_pred_proba = under_model.predict_proba(X_test)[:, 1]\n",
    "under_auc = roc_auc_score(y_test, under_pred_proba)\n",
    "\n",
    "print(f\"Under-sampled model AUC: {under_auc:.4f}\")\n",
    "print(f\"Improvement: {under_auc - baseline_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "print(\"\\n=== COMPARISON OF IMBALANCED DATA METHODS ===\")\n",
    "\n",
    "methods = {\n",
    "    'Baseline (Imbalanced)': baseline_auc,\n",
    "    'SMOTE': smote_auc,\n",
    "    'Class Weights': weighted_auc,\n",
    "    'Under-sampling': under_auc\n",
    "}\n",
    "\n",
    "for method, auc in methods.items():\n",
    "    print(f\"{method:<20}: {auc:.4f}\")\n",
    "\n",
    "# Best method\n",
    "best_method = max(methods.items(), key=lambda x: x[1])\n",
    "print(f\"\\nBest method: {best_method[0]} (AUC: {best_method[1]:.4f})\")\n",
    "\n",
    "# Visualize ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "method_probabilities = {\n",
    "    'Baseline': baseline_pred_proba,\n",
    "    'SMOTE': smote_pred_proba,\n",
    "    'Weighted': weighted_pred_proba,\n",
    "    'Under-sampled': under_pred_proba\n",
    "}\n",
    "\n",
    "for name, proba in method_probabilities.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    auc_score = roc_auc_score(y_test, proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.7, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves: Imbalanced Data Methods')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Selection\n",
    "\n",
    "Feature selection helps improve model performance and interpretability by selecting the most relevant features.\n",
    "\n",
    "### Methods:\n",
    "1. **Filter Methods**: Statistical tests (chi-square, correlation)\n",
    "2. **Wrapper Methods**: Use model performance (RFE, forward/backward selection)\n",
    "3. **Embedded Methods**: Feature selection during training (Lasso, Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "print(\"=== FEATURE SELECTION ===\")\n",
    "\n",
    "# Method 1: SelectKBest with chi-square\n",
    "# Note: chi-square requires non-negative features\n",
    "X_train_pos = X_train - X_train.min() + 1  # Make all features positive\n",
    "X_test_pos = X_test - X_train.min() + 1\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train_pos, y_train)\n",
    "X_test_selected = selector.transform(X_test_pos)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = [feature_names[i] for i in selector.get_support(indices=True)]\n",
    "print(f\"Selected features (chi-square): {selected_features}\")\n",
    "\n",
    "# Train model with selected features\n",
    "selected_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "selected_model.fit(X_train_selected, y_train)\n",
    "selected_pred_proba = selected_model.predict_proba(X_test_selected)[:, 1]\n",
    "selected_auc = roc_auc_score(y_test, selected_pred_proba)\n",
    "\n",
    "print(f\"Selected features model AUC: {selected_auc:.4f}\")\n",
    "print(f\"Original model AUC: {rf_auc:.4f}\")\n",
    "print(f\"Difference: {selected_auc - rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Recursive Feature Elimination (RFE)\n",
    "print(\"\\n=== RECURSIVE FEATURE ELIMINATION ===\")\n",
    "\n",
    "# Use RFE with Random Forest\n",
    "estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get selected features\n",
    "rfe_features = [feature_names[i] for i in range(len(feature_names)) if rfe.support_[i]]\n",
    "print(f\"Selected features (RFE): {rfe_features}\")\n",
    "\n",
    "# Transform data\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# Train model with RFE features\n",
    "rfe_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfe_model.fit(X_train_rfe, y_train)\n",
    "rfe_pred_proba = rfe_model.predict_proba(X_test_rfe)[:, 1]\n",
    "rfe_auc = roc_auc_score(y_test, rfe_pred_proba)\n",
    "\n",
    "print(f\"RFE model AUC: {rfe_auc:.4f}\")\n",
    "print(f\"Difference from original: {rfe_auc - rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Congratulations! You've mastered advanced machine learning techniques. Here's what you learned:\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "1. **Ensemble Methods**: Combining models for better performance\n",
    "2. **Bagging**: Random Forest and bootstrap aggregating\n",
    "3. **Boosting**: Gradient Boosting, XGBoost, LightGBM\n",
    "4. **Voting**: Combining different algorithm types\n",
    "5. **Hyperparameter Tuning**: Grid search vs random search\n",
    "6. **Imbalanced Data**: SMOTE, class weights, resampling\n",
    "7. **Feature Selection**: Choosing the most relevant features\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- Building and combining ensemble models\n",
    "- Optimizing hyperparameters systematically\n",
    "- Handling imbalanced datasets effectively\n",
    "- Selecting features to improve model performance\n",
    "- Understanding trade-offs between different techniques\n",
    "- Implementing advanced ML algorithms\n",
    "\n",
    "### Best Practices:\n",
    "- Always validate ensemble improvements with cross-validation\n",
    "- Use random search for initial exploration, grid search for fine-tuning\n",
    "- Consider class imbalance early in your modeling process\n",
    "- Feature selection should be done within cross-validation\n",
    "- Monitor overfitting, especially with boosting algorithms\n",
    "- Combine multiple techniques for optimal results\n",
    "\n",
    "### When to Use Each Technique:\n",
    "- **Random Forest**: General-purpose, good baseline, feature importance\n",
    "- **Gradient Boosting**: High accuracy, handles missing values\n",
    "- **XGBoost/LightGBM**: Competitions, large datasets, best performance\n",
    "- **Voting**: Combining diverse algorithms\n",
    "- **SMOTE**: Moderate imbalance, sufficient data\n",
    "- **Class Weights**: Severe imbalance, limited data\n",
    "\n",
    "### Real-world Applications:\n",
    "- Fraud detection (imbalanced data, ensemble methods)\n",
    "- Medical diagnosis (high accuracy, interpretability)\n",
    "- Marketing campaigns (customer targeting, feature selection)\n",
    "- Financial modeling (risk assessment, robust predictions)\n",
    "- Competition machine learning (XGBoost, stacking)\n",
    "\n",
    "### Next Steps:\n",
    "In the next week, we'll explore neural networks and deep learning, building on the foundation you've established with traditional machine learning techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
