{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Introduction to Supervised Learning and Linear Regression\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand ML concepts and terminology\n",
    "- Learn supervised vs unsupervised learning\n",
    "- Implement first ML algorithms\n",
    "- Understand model evaluation\n",
    "\n",
    "## Topics Covered:\n",
    "- Types of ML: supervised, unsupervised, reinforcement\n",
    "- Linear regression singular\n",
    "- Feature Engineering\n",
    "- Feature scaling and selection\n",
    "- Model evaluation metrics (RMSE)\n",
    "- Train/validation/test splits\n",
    "- Cross-validation\n",
    "- Overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, validation_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Machine Learning?\n",
    "\n",
    "Machine Learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every scenario.\n",
    "\n",
    "### Key Terminology:\n",
    "- **Algorithm**: The mathematical procedure used to make predictions\n",
    "- **Model**: The output of an algorithm after training on data\n",
    "- **Features**: Input variables used to make predictions\n",
    "- **Target/Label**: The output variable we want to predict\n",
    "- **Training**: The process of teaching the algorithm using historical data\n",
    "- **Prediction**: Using the trained model to make forecasts on new data\n",
    "- **Overfitting**: When a model learns the training data too well and doesn't generalize\n",
    "- **Underfitting**: When a model is too simple to capture the underlying pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Machine Learning\n",
    "\n",
    "### 2.1 Supervised Learning\n",
    "Learning with labeled data (input-output pairs)\n",
    "\n",
    "**Classification**: Predicting categories/classes\n",
    "- Examples: Email spam detection, image recognition, medical diagnosis\n",
    "- Target variable: Discrete/categorical\n",
    "\n",
    "**Regression**: Predicting continuous numerical values\n",
    "- Examples: House price prediction, stock price forecasting, temperature prediction\n",
    "- Target variable: Continuous/numerical\n",
    "\n",
    "### 2.2 Unsupervised Learning\n",
    "Learning patterns in data without labeled examples\n",
    "- Examples: Customer segmentation, anomaly detection, data compression\n",
    "- No target variable provided\n",
    "\n",
    "### 2.3 Reinforcement Learning\n",
    "Learning through interaction with an environment using rewards and penalties\n",
    "- Examples: Game playing, autonomous vehicles, recommendation systems\n",
    "- Learning through trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset to demonstrate supervised learning\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic house price data\n",
    "n_samples = 200\n",
    "house_size = np.random.normal(1500, 500, n_samples)  # Square feet\n",
    "bedrooms = np.random.randint(1, 6, n_samples)  # Number of bedrooms\n",
    "age = np.random.randint(0, 50, n_samples)  # House age in years\n",
    "\n",
    "# Create a realistic relationship for house prices\n",
    "price = (house_size * 120 + bedrooms * 15000 - age * 1000 + \n",
    "         np.random.normal(0, 20000, n_samples))\n",
    "\n",
    "# Create DataFrame\n",
    "house_data = pd.DataFrame({\n",
    "    'Size_SqFt': house_size,\n",
    "    'Bedrooms': bedrooms,\n",
    "    'Age_Years': age,\n",
    "    'Price': price\n",
    "})\n",
    "\n",
    "print(\"House Price Dataset:\")\n",
    "print(house_data.head())\n",
    "print(f\"\\nDataset shape: {house_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Size vs Price\n",
    "axes[0, 0].scatter(house_data['Size_SqFt'], house_data['Price'], alpha=0.6, color='blue')\n",
    "axes[0, 0].set_xlabel('Size (Square Feet)')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].set_title('House Size vs Price')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bedrooms vs Price\n",
    "bedroom_avg = house_data.groupby('Bedrooms')['Price'].mean()\n",
    "axes[0, 1].bar(bedroom_avg.index, bedroom_avg.values, color='green', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Number of Bedrooms')\n",
    "axes[0, 1].set_ylabel('Average Price ($)')\n",
    "axes[0, 1].set_title('Bedrooms vs Average Price')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Age vs Price\n",
    "axes[1, 0].scatter(house_data['Age_Years'], house_data['Price'], alpha=0.6, color='red')\n",
    "axes[1, 0].set_xlabel('Age (Years)')\n",
    "axes[1, 0].set_ylabel('Price ($)')\n",
    "axes[1, 0].set_title('House Age vs Price')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Price distribution\n",
    "axes[1, 1].hist(house_data['Price'], bins=25, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Price ($)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Price Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix = house_data.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Regression: Your First ML Algorithm\n",
    "\n",
    "Linear regression is one of the simplest and most interpretable machine learning algorithms. It assumes a linear relationship between the input features and the target variable.\n",
    "\n",
    "### Mathematical Foundation:\n",
    "For simple linear regression (one feature):\n",
    "```\n",
    "y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ\n",
    "```\n",
    "\n",
    "For multiple linear regression:\n",
    "```\n",
    "y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ\n",
    "```\n",
    "\n",
    "Where:\n",
    "- y = target variable (price)\n",
    "- Œ≤‚ÇÄ = intercept\n",
    "- Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô = coefficients\n",
    "- x‚ÇÅ, x‚ÇÇ, ..., x‚Çô = features\n",
    "- Œµ = error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression - Single Feature\n",
    "print(\"=== SIMPLE LINEAR REGRESSION ===\")\n",
    "\n",
    "# Use only house size as feature\n",
    "X_simple = house_data[['Size_SqFt']]\n",
    "y = house_data['Price']\n",
    "\n",
    "# Split the data\n",
    "X_train_simple, X_test_simple, y_train, y_test = train_test_split(\n",
    "    X_simple, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train the model\n",
    "simple_model = LinearRegression()\n",
    "simple_model.fit(X_train_simple, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_simple = simple_model.predict(X_test_simple)\n",
    "\n",
    "# Print model parameters\n",
    "print(f\"Intercept (Œ≤‚ÇÄ): ${simple_model.intercept_:,.2f}\")\n",
    "print(f\"Coefficient (Œ≤‚ÇÅ): ${simple_model.coef_[0]:.2f} per square foot\")\n",
    "print(f\"\\nModel equation: Price = ${simple_model.intercept_:,.2f} + ${simple_model.coef_[0]:.2f} √ó Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the simple linear regression\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X_test_simple, y_test, alpha=0.6, color='blue', label='Actual Prices')\n",
    "plt.scatter(X_test_simple, y_pred_simple, alpha=0.6, color='red', label='Predicted Prices')\n",
    "\n",
    "# Plot regression line\n",
    "X_line = np.linspace(X_simple.min(), X_simple.max(), 100).reshape(-1, 1)\n",
    "y_line = simple_model.predict(X_line)\n",
    "plt.plot(X_line, y_line, color='green', linewidth=2, label='Regression Line')\n",
    "\n",
    "plt.xlabel('Size (Square Feet)')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Simple Linear Regression: House Size vs Price')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display metrics\n",
    "rmse_simple = np.sqrt(mean_squared_error(y_test, y_pred_simple))\n",
    "mae_simple = mean_absolute_error(y_test, y_pred_simple)\n",
    "r2_simple = r2_score(y_test, y_pred_simple)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"RMSE: ${rmse_simple:,.2f}\")\n",
    "print(f\"MAE: ${mae_simple:,.2f}\")\n",
    "print(f\"R¬≤ Score: {r2_simple:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression uses multiple features to make predictions. This often provides better performance than using a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression - All Features\n",
    "print(\"=== MULTIPLE LINEAR REGRESSION ===\")\n",
    "\n",
    "# Use all features\n",
    "X_multiple = house_data[['Size_SqFt', 'Bedrooms', 'Age_Years']]\n",
    "\n",
    "# Split the data\n",
    "X_train_multiple, X_test_multiple, y_train, y_test = train_test_split(\n",
    "    X_multiple, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train the model\n",
    "multiple_model = LinearRegression()\n",
    "multiple_model.fit(X_train_multiple, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_multiple = multiple_model.predict(X_test_multiple)\n",
    "\n",
    "# Print model parameters\n",
    "print(f\"Intercept (Œ≤‚ÇÄ): ${multiple_model.intercept_:,.2f}\")\n",
    "print(\"\\nCoefficients:\")\n",
    "for feature, coef in zip(X_multiple.columns, multiple_model.coef_):\n",
    "    print(f\"  {feature}: {coef:.2f}\")\n",
    "\n",
    "# Model equation\n",
    "print(f\"\\nModel equation:\")\n",
    "print(f\"Price = ${multiple_model.intercept_:,.2f} + \")\n",
    "print(f\"        ${multiple_model.coef_[0]:.2f} √ó Size + \")\n",
    "print(f\"        ${multiple_model.coef_[1]:,.2f} √ó Bedrooms + \")\n",
    "print(f\"        ${multiple_model.coef_[2]:.2f} √ó Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "rmse_multiple = np.sqrt(mean_squared_error(y_test, y_pred_multiple))\n",
    "mae_multiple = mean_absolute_error(y_test, y_pred_multiple)\n",
    "r2_multiple = r2_score(y_test, y_pred_multiple)\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(f\"{'Metric':<15} {'Simple':<15} {'Multiple':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'RMSE':<15} ${rmse_simple:<14,.0f} ${rmse_multiple:<14,.0f} {((rmse_simple-rmse_multiple)/rmse_simple)*100:<14.1f}%\")\n",
    "print(f\"{'MAE':<15} ${mae_simple:<14,.0f} ${mae_multiple:<14,.0f} {((mae_simple-mae_multiple)/mae_simple)*100:<14.1f}%\")\n",
    "print(f\"{'R¬≤ Score':<15} {r2_simple:<15.3f} {r2_multiple:<15.3f} {((r2_multiple-r2_simple)/r2_simple)*100:<14.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Validation/Test Splits\n",
    "\n",
    "Proper data splitting is crucial for reliable model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test Split\n",
    "print(\"=== TRAIN/VALIDATION/TEST SPLIT ===\")\n",
    "\n",
    "# First split: separate test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_multiple, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation from remaining data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42  # 0.25 of 0.8 = 0.2 of total\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(X_multiple)}\")\n",
    "print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X_multiple)*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(X_val)} ({len(X_val)/len(X_multiple)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X_multiple)*100:.1f}%)\")\n",
    "\n",
    "# Train model on training set\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on all three sets\n",
    "train_pred = model.predict(X_train)\n",
    "val_pred = model.predict(X_val)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "train_r2 = r2_score(y_train, train_pred)\n",
    "val_r2 = r2_score(y_val, val_pred)\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Training RMSE: ${train_rmse:,.2f}, R¬≤: {train_r2:.3f}\")\n",
    "print(f\"Validation RMSE: ${val_rmse:,.2f}, R¬≤: {val_r2:.3f}\")\n",
    "print(f\"Test RMSE: ${test_rmse:,.2f}, R¬≤: {test_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation\n",
    "\n",
    "Cross-validation provides a more robust estimate of model performance by using multiple train/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation\n",
    "print(\"=== CROSS-VALIDATION ===\")\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv_scores = cross_val_score(LinearRegression(), X_multiple, y, cv=5, scoring='r2')\n",
    "cv_rmse_scores = cross_val_score(LinearRegression(), X_multiple, y, cv=5, \n",
    "                                scoring='neg_mean_squared_error')\n",
    "cv_rmse_scores = np.sqrt(-cv_rmse_scores)\n",
    "\n",
    "print(f\"5-Fold Cross-Validation Results:\")\n",
    "print(f\"R¬≤ Scores: {cv_scores}\")\n",
    "print(f\"Mean R¬≤: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "print(f\"\\nRMSE Scores: {cv_rmse_scores}\")\n",
    "print(f\"Mean RMSE: ${cv_rmse_scores.mean():,.2f} (+/- ${cv_rmse_scores.std() * 2:,.2f})\")\n",
    "\n",
    "# Visualize cross-validation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# R¬≤ scores\n",
    "axes[0].bar(range(1, 6), cv_scores, alpha=0.7, color='skyblue')\n",
    "axes[0].axhline(y=cv_scores.mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {cv_scores.mean():.3f}')\n",
    "axes[0].set_xlabel('Fold')\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].set_title('Cross-Validation R¬≤ Scores')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE scores\n",
    "axes[1].bar(range(1, 6), cv_rmse_scores, alpha=0.7, color='lightcoral')\n",
    "axes[1].axhline(y=cv_rmse_scores.mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: ${cv_rmse_scores.mean():,.0f}')\n",
    "axes[1].set_xlabel('Fold')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Cross-Validation RMSE Scores')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Overfitting and Underfitting\n",
    "\n",
    "Understanding the bias-variance tradeoff is crucial for building good models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting with polynomial features\n",
    "print(\"=== OVERFITTING AND UNDERFITTING ===\")\n",
    "\n",
    "# Use only size feature for simplicity\n",
    "X_simple = house_data[['Size_SqFt']].values\n",
    "y_simple = house_data['Price'].values\n",
    "\n",
    "# Split data\n",
    "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(\n",
    "    X_simple, y_simple, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Test different polynomial degrees\n",
    "degrees = [1, 2, 3, 5, 10, 15]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_simple)\n",
    "    X_test_poly = poly_features.transform(X_test_simple)\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train_simple)\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = model.score(X_train_poly, y_train_simple)\n",
    "    test_score = model.score(X_test_poly, y_test_simple)\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "    print(f\"Degree {degree}: Train R¬≤ = {train_score:.3f}, Test R¬≤ = {test_score:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_scores, 'o-', label='Training Score', color='blue')\n",
    "plt.plot(degrees, test_scores, 'o-', label='Testing Score', color='red')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('Overfitting vs Underfitting')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Low degrees (1-2): May underfit - too simple to capture patterns\")\n",
    "print(\"- High degrees (10+): May overfit - too complex, memorizes training data\")\n",
    "print(\"- Optimal degree: Where test score is maximized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation Metrics Deep Dive\n",
    "\n",
    "Understanding different evaluation metrics helps choose the right model for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation metrics\n",
    "print(\"=== EVALUATION METRICS EXPLAINED ===\")\n",
    "\n",
    "# Use our multiple regression model\n",
    "y_pred = y_pred_multiple\n",
    "\n",
    "# Calculate various metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:,.0f}\")\n",
    "print(f\"  - Measures average squared difference between actual and predicted\")\n",
    "print(f\"  - Penalizes large errors more heavily\")\n",
    "print(f\"  - Units: squared dollars\")\n",
    "\n",
    "print(f\"\\nRoot Mean Squared Error (RMSE): ${rmse:,.2f}\")\n",
    "print(f\"  - Square root of MSE\")\n",
    "print(f\"  - Same units as target variable (dollars)\")\n",
    "print(f\"  - Easier to interpret than MSE\")\n",
    "\n",
    "print(f\"\\nMean Absolute Error (MAE): ${mae:,.2f}\")\n",
    "print(f\"  - Average absolute difference between actual and predicted\")\n",
    "print(f\"  - Less sensitive to outliers than RMSE\")\n",
    "print(f\"  - Same units as target variable\")\n",
    "\n",
    "print(f\"\\nR¬≤ Score (Coefficient of Determination): {r2:.3f}\")\n",
    "print(f\"  - Proportion of variance in target explained by model\")\n",
    "print(f\"  - Range: -‚àû to 1 (1 is perfect, 0 is no better than mean)\")\n",
    "print(f\"  - {r2*100:.1f}% of price variation is explained by our model\")\n",
    "\n",
    "print(f\"\\nMean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"  - Average percentage error between actual and predicted\")\n",
    "print(f\"  - Useful for understanding relative error size\")\n",
    "print(f\"  - Our model is off by an average of {mape:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practice Exercises\n",
    "\n",
    "Now it's your turn to practice what you've learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 1: Feature Engineering\n",
    "# Create a new feature that represents price per square foot\n",
    "# and train a model using this feature along with the original features\n",
    "\n",
    "# Your code here\n",
    "# Solution:\n",
    "house_data_extended = house_data.copy()\n",
    "house_data_extended['Price_per_SqFt'] = house_data_extended['Price'] / house_data_extended['Size_SqFt']\n",
    "house_data_extended['Room_Size'] = house_data_extended['Size_SqFt'] / house_data_extended['Bedrooms']\n",
    "\n",
    "# Train model with engineered features\n",
    "X_engineered = house_data_extended[['Size_SqFt', 'Bedrooms', 'Age_Years', 'Room_Size']]\n",
    "X_train_eng, X_test_eng, y_train_eng, y_test_eng = train_test_split(\n",
    "    X_engineered, house_data_extended['Price'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_eng = LinearRegression()\n",
    "model_eng.fit(X_train_eng, y_train_eng)\n",
    "y_pred_eng = model_eng.predict(X_test_eng)\n",
    "\n",
    "print(f\"Model with engineered features - R¬≤: {r2_score(y_test_eng, y_pred_eng):.3f}\")\n",
    "print(f\"Original model - R¬≤: {r2_multiple:.3f}\")\n",
    "print(f\"Improvement: {((r2_score(y_test_eng, y_pred_eng) - r2_multiple) / r2_multiple * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 2: Model Comparison\n",
    "# Compare the performance of different train/test split ratios\n",
    "\n",
    "# Your code here\n",
    "# Solution:\n",
    "split_ratios = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "results = []\n",
    "\n",
    "for ratio in split_ratios:\n",
    "    X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
    "        X_multiple, y, test_size=ratio, random_state=42\n",
    "    )\n",
    "    \n",
    "    model_split = LinearRegression()\n",
    "    model_split.fit(X_train_split, y_train_split)\n",
    "    y_pred_split = model_split.predict(X_test_split)\n",
    "    \n",
    "    r2_split = r2_score(y_test_split, y_pred_split)\n",
    "    results.append(r2_split)\n",
    "    \n",
    "    print(f\"Test size: {ratio*100:.0f}% - R¬≤: {r2_split:.3f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([r*100 for r in split_ratios], results, 'o-', color='blue', linewidth=2, markersize=8)\n",
    "plt.xlabel('Test Set Size (%)')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('Model Performance vs Test Set Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Congratulations! You've completed your introduction to supervised learning and linear regression. Here's what you learned:\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "1. **Machine Learning Types**: Supervised, unsupervised, and reinforcement learning\n",
    "2. **Linear Regression**: Simple and multiple linear regression\n",
    "3. **Feature Engineering**: Creating new features to improve model performance\n",
    "4. **Model Evaluation**: RMSE, MAE, R¬≤, and MAPE metrics\n",
    "5. **Data Splitting**: Train/validation/test splits and cross-validation\n",
    "6. **Overfitting/Underfitting**: Understanding the bias-variance tradeoff\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- Building and interpreting linear regression models\n",
    "- Evaluating model performance using multiple metrics\n",
    "- Implementing proper data splitting strategies\n",
    "- Understanding overfitting and underfitting\n",
    "- Creating and engineering features for better predictions\n",
    "\n",
    "### Next Week Preview: Multiple Linear Regression and Fine Tuning\n",
    "- Multiple Linear Regression concepts\n",
    "- Ridge Regression for regularization\n",
    "- Lasso Regression for feature selection\n",
    "- ElasticNet combining Ridge and Lasso\n",
    "- Model selection and hyperparameter tuning\n",
    "\n",
    "### Best Practices to Remember:\n",
    "- Always split your data before training\n",
    "- Use cross-validation for robust performance estimates\n",
    "- Start with simple models before adding complexity\n",
    "- Feature engineering can significantly improve performance\n",
    "- Understand your evaluation metrics and choose appropriate ones\n",
    "- Watch out for overfitting with complex models\n",
    "\n",
    "Great job on completing your first machine learning algorithms! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
